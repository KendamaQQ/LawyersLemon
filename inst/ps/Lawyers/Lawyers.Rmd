
```{r 'check_ps', include=FALSE}

user.name = '' # set to your user name

# To check your problem set, run the 
# RStudio Addin 'Check Problemset'

# Alternatively run the following lines
library(RTutor)
ps.dir = getwd() # directory of this file
ps.file = 'Lawyers.Rmd' # name of this file
check.problem.set('Lawyers', ps.dir, ps.file, user.name=user.name, reset=FALSE)
```


# Selection and Success of Lawyers

Author: Artemij Cadov






### Hello there!

Since the landmark case [*Gideon v. Wainwright*](https://supreme.justia.com/cases/federal/us/372/335/), and it's application on the Sixth Amendment, states are required to provide an attorney to defendants in criminal cases who are unable to afford their own counsel. Thereupon, two different approaches for defending indigent criminal accused emerged: public defender systems and assigned counsel programs. While public defenders are government employees who only represent indigent clients, assigned counsels are independent private attorneys who enroll into a selection pool. Hence, public counsels are typically paid a salary rather than at an hourly rate. By contrast assigned attorneys (also called Criminal Justics Act (CJA) panel attorneys) are generally paid by the local government on a case-by-case basis after their services are rendered.  While in 79 % of all US jurisdictions both approaches are intertwined, this problem set just focuses on Bexar County, Texas, where assigned attorneys are used almost exclusively. Obviously, defendants with the means can always hire their own counsel for the purpose of criminal defense. In 2014, only about 24 % of cases in Bexar County were able to retain their own counsel. The ability of some defendants to hire their own attorneys, coupled with the fact that many defendants do not have the means to pay for legal representation, raises, inter alia, the question of attorney effectiveness:

---

#! addon__quiz__entry question

---


This is an interactive problem set, which is part of my master's thesis at Ulm University. 
It examines, why assigned as opposed to retained attorneys achieve relatively worse outcomes for their clients.
But there is far more information out there, that we can consider in answering this question, which is our **research question**.
Like always when learning new things, answering questions leads initially to even more questions. Like, what is the appropirate model to 
answer the research question, which covariates shall be included in the model and how to implement it in R?
Don't worry, if you are interested by this questions, this problem set will ease the way to answer them all. 

The analysis is based on the article ["Is Your Lawyer a Lemon? Incentives and Selection in the Public Provision of Criminal Defense"](https://www.mitpressjournals.org/doi/abs/10.1162/rest_a_00891) by Amanda Agan, Matthew Freedman and Emily Owens (2019). Be aware, that there is also an earlier version, respectively [working paper](https://www.nber.org/papers/w24579) out there. I will refer to this article simply as "Agan et al." or "the authors" from now on.

You will derive most results interactively using the programming language R. This means that you have to enter your own R code from time to time. This way, you can enhance your R programming skills while reproducing the results from an interesting economic article. I will explain some R functions when you need them, but you are expected to have basic R skills. If you need an introduction to programming in R, you can, for example, have a look at [R for Beginners](https://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf). Below, you can find more information on how to solve the problem set. If you are ready, proceed to Exercise 1 and get started!

---

## Exercise Content


1. Getting Started

    1.1 Basic Summarys and Working with R

    1.2 Descriptive Statistics
    
2. Investigate Lawyer Heterogeneity
  
    2.1 Unconditional Assigned Counsel Penalty
    
    2.2 Panel Data and Causality
    
    2.3 Assigned Counsel Penalty

3. Decomposition of Assigned Counsel Penalty

    3.1 Case Characteristics and Adverse Selection
    
    3.2 Matching Preferances
    
    3.3 Moral Hazard

5. Investigating Recidivism

6. Related Literature

7. Conclusion

8. References


---

### How to solve this problem set


Solving the problem set is pretty straightforward. Here is a short explanation of the elements you may encounter.

- Code Chunks: They are used to enter and run R code. You have to solve a code chunk before you can interact with the following one. To interact with code chunks, there are several buttons:

    + `edit`: After pressing this button, you can enter your R code into the chunk. You always have to press the edit button on the first code chunk of an exercise.
    + `check`: After entering your solution, you have to press check to run your code and check whether your answer is correct. Sometimes, the correct answer is already entered in the chunk, so you only have to press check.
    + `hint`: If you need help solving the chunk, you can press this button to get a hint.
    + `run chunk`: This button runs the code chunk without checking whether it is correct.
    + `data`: This button is a link to the Data Explorer, in which you can view data sets and look at variable descriptions.
    + `solution`: Pressing this button enters the sample solution into the code chunk.

- Tasks: Whenever you are required to interact with a code chunk, the text which succeeds "**Task:**" will tell you what you have to do.

- Quizzes: It is not necessary to solve the quizzes to be able to continue with the problem set, but they can contribute to your overall understanding of the topic.

- Info Boxes: They contain additional information which is not necessary for solving the problem set, for example explanations of R functions or mathematical proofs.

To navigate through the problem set, you can use the button `Go to next exercise...` at the bottom of the page or the menu bar at the top. It is possible to solve each exercise without solving the previous ones, however you are encouraged to solve the exercises in the specified order as they follow a didactic structure and frequently use data sets which have been derived and analyzed in previous exercises.


## Exercise 1 -- Getting Started

We will strat off this problem set by having a look at basic data manipulation techniques in exercise 1.1. In parallel you have the possibility to get familiar with the data by examining some summary statistics. In Exercise 1.2 we will then replicate and look at some descriptive statistics from the authors paper. Overall, this exercise aims at preparing you for the more advanced exercises afterwards by combining both, programming knowledge and information on the topic that we study. 

## Exercise 1.1 -- Basic Summarys and Working with R

The first necessary step for every sophisticated analysis is to explore the data. Agan et al. merged multiple data sources into one 
data set, which consists of *64209 observations* on over 600 variables. This might seem overwhelming at first place, but be appeased with
the fact, that our analysis necessitates just a *subsample of 60 variables*. The data provides information about cases, whereat *each observation corresponds to a case*. Each case contains defendant and attorney characteristics as well as information on the defendants block group, assembeled
from the ACS 2009-2013 (American Community Survey). 

Without further ado, let's now have a look into the actual data.

**Task:**
Use the `readRDS` command to load the data set and store it in a variable called `Lawyers`.

```{r "1_1"}
# Reading the data can take a little bit of time, so be patient.
Lawyers <- ___("Lawyers.rds")
```



I'm certain, that you are familiar with the `summary()` function. But there are far more descriptive functions out there. The *summarytools package* is therefore a great startingpoint for our analysis. One of it's function is `dfSummary()`, which provides a nice, but quiet verbose data description. So let's first subset our sample to the variables: *guilty* and *appointed*. Albeit, base R allows for subsetting, `dplyr` 
(or/and `tidyverse`) render possible an intuitive and simple way for all kinds of data manipulation including subsetting. The `%>%` (pipe) is a nice way of writing clean and fancy code. If you are not familiar with it check out the info box below for a short explenation. 

**Task:** 
Save the desired variables in a vector subsetting and use it to subset the data saved in L. 
Save the result in a variable subsetted. Use the `dplyr` package to do so.

```{r "1_1__2"}
# This is the vector, which contains the desired variables:
subsetting <- c("guilty", "appointed")
# Now use the vector to subset Lawyers
subsetted <- Lawyers %>% ___(all_of(___))
# Shows the result:
subsetted
```

info("dplyr") # Run this line (Strg-Enter) to show info


info("%>%") # Run this line (Strg-Enter) to show info


Both, *guilty* and *appointed* are dummy variables, yielding one if true, e.g. the defendant was *guilty* if the variable equals one, and acquitted otherwise. This two variables are main components of our analysis. On the one hand, *guilty* is one of the four main *outcome/endogen* variables used by the authors, on the other hand, *appointed* comprises the information critical for *identification*.
 
**Task:** 
Use the `dfSummary()` function to show the summary statistics for the *subsetted* data. You can look at the Note beneath for the (detailed) solution and additional information.

```{r "1_1__3",result='asis'}
# Enter your code here.
```

#! start_note "dfSummary()"
The most simple use of `dfSummary()` is to pass just the data as the single argument. But if you type ?dfSummary() into the console you can dive deeper in the numerous arguments `dfSummary()` provides. The following chunck shows an elaborate solution to the task, where for example the graph column is excluded because it is unusable for a binary variable. Just remove the hashtag and run the code.

```{r "1_1__4",optional=TRUE}
# dfSummary(subsetted, round.digits = 3, varnumbers = F, labels.col = F, valid.col = F, graph.col = F, justify = "c")
```

#! end_note


The summary shows information such as the number of observations, data type of the variables and wheater missing values are present. 
The table reveals that 63 % of fellony cases in Bexar County between 2005 and 2014 were negotiated by assigned counsels and 48 % were convicted.
To know how many people were convicted overall isn't very informative, if we are interested in studying differences between two groups (retained vs. assigned). Therefore let's compute the conviction rate based on counsel appointment. 

**Task:**
Use `group_by` and `summarise` to compute the conviction_rate for each group of attorney type. 
Also compute the number of cases in each group.

```{r "1_1__5"}
Lawyers %>% 
group_by(appointed) %>% 
summarise(conviction_rate=___,
          observations=___) 
```

Instead of using `dfSummary` in the case of binary/categorical variables, **summarytools** provides also the `freq()` function, allowing for nice frequency tables.

**Task:** 
Use *subsetted* and apply the `freq()` function to each variable with the `lapply()` function. 
See the info box, if you are not familiar with `lapply()`.

```{r "1_1__6"}
# fill in the second argument of the lapply function
lapply(subsetted, ___, report.nas=F)
```

info("`lapply()`") # Run this line (Strg-Enter) to show info

As you see, 63 % of all attorneys were appointed to the respective case. This is nice to know, but rarely helps to answer our research question. 
Hence, after this short but instructive introduction, let's replicate some descriptive statistics from the paper to dive beyond the surface of the topic.  






## Exercise 1.2 -- Descriptive Statistics

It is necessary to read in the data for each exercise again. Just run the beneath code.
```{r "1_2"}
Lawyers <- readRDS("Lawyers.rds")

```

We want also to obtain some information regarding the attorneys in out data set. For example, how many distinct counsels does the data comprise? How many of them have served both as assigned and retained attorneys? Are there systematic differences in attorney characteristics between counsels who handle assigned as opposed to retained cases? These are vital issues, because they give us a glimpse on the answer to our **research question**. Let's tackle them one by one. 

**Task:**
At first, compute the number of distinct attorneys and the number of cases each of them handled. Each attorney has an unique numerical code, attributed by the State Bar, which is saved in the variable *attybarcard*. Use this variable as argument to the function `count()`, which is also shipped with `dplyr` and is more handy for such tasks, than building a pipe yourself. 

```{r "1_2__2",optional=TRUE}
# Just follow the instructions in the task and complete the line
counting <- Lawyers %>% ___

# Print results
kable(counting) %>% kable_styling("striped", "hover", "condensed", position= "left", full_width = F) %>% scroll_box(height = "300px")
```

info("count()") # Run this line (Strg-Enter) to show info


Apparently the number of cases differs dramatically from one attorney to another. 

**Task:**
Even more interesting is to find out, how many of the attorneys resolved both, appointed and retained, cases. Enter the right condition into `filter()` to achieve that. 
*Note:* There are many ways to accomplish the desired result. The way used here is, first to compute for each attorney the percentage of assigned cases, and secondly to filter out all whom doesn't fullfill the condition. Finally, the number of rows represents the result.

```{r "1_2__4",optional=TRUE}
both <- Lawyers %>% group_by(attybarcard) %>% 
  summarise(ratio.both = mean(appointed)) %>% 
  filter(___)

# Produce Output
kable(both) %>% 
  kable_styling("striped", "hover", "condensed", position= "left", full_width = F) %>% scroll_box(height = "300px")
```

So there are 492 attorneys, who handle both type of cases. But let's look at overall characteristics between retained and assigned cases.


**Task:**
Simply click check and look at the tables. You can switch between them either by wiping with your mouse or clicking on the radio buttons beneath.

```{r "1_2__5",results='asis'}
includeHTML("index.html")
```

<br>
<br>
Please notice that this cube of tables following *figure 2* from the paper and focuses on the most remarkable descriptives. All values in the cube represent means. Again, have a look at *figure 2*, if you are also interested in the standard deviation. Obtaining this numbers is by no means difficult, hence, it's not really necessary to solve an auxillary chunck. Nevertheless let's exemplify one value, e.g. the white characteristic in the defendants table. As we already calculated in **Exercise 1.1** there are 23559 cases, where the attorney was retained and 40650 cases, where the attorney was assigned. To determine the percentage, we simply calculate the number of white defendants in each group (retained: 6650, appointed: 9124) and divide it by the total case number in each group or use the `mean()` function plainly. All values in all three tables are calculated this way. Let's proceed with discussing the figures.

Looking at the first Table it can be noticed at a glance that in relative terms attorneys get retained more often by white defendants compared to black defendants. While the hispanic share is paramount in both groups, there can't be detected a similar difference as in the other two defendant characteristics. Remember, at this point we can't and shouldn't aim to answer what drives this (missing) differences, we just might get a feeling, that there is some hidden explenation yet to unravel. Indeed, we can easily explain why the share of hispanic defendants is in both groups so large. Bexar County comprises San Antonio, which is the seat of the county and place to 1.7 million people (in 2010). The County is ethnically diverse, in 2010 59.1% identified as Hispanic, 29.5% as White and 8.2% of the population identified as African American. In this context, the percentages in the retained group mirror the respective population fractions, with a slightly elevated value on the side of African American defendants. Whereas in the assigned group, despite hispanic defendants, black defendants are overrepresented in favor of white defendants. 
Both, previous convictions and poverty rate, are drastically higher in the appointed group aswell. Let's rotate the cube and look at the second table.

Apparently, the share of male attorneys is higher, when the counsel is retained. The years of experience is also higher in the retained group and the amount of previous cases is nearly doubled contrary to the average value in the assigned group. Computing the previous cases per year, the retained group (32) is still ahead of the assigned group (22). Further, there seems so be a preference towards retaining an attorney of same ethnical background in the privat market. Finally, let's look at the *outcome characteristics* in the last table. 

The authors analyzed in their paper mainly four outcome variables: *Convicted*, *Dismissed*, *Deferred Adjudication* and *Incarcerated given Conviction*. In this problem set we will mainly focus on *Convicted*. The proceeding is almost identical, the only changing factor in the models we will study is the outcome variable itself. Of course, we will provide the results to the other three variables subsequently, e.g. in an info box. But now let's eventually turn to the figures. By now, you may already expected, that the outcome of felony cases is detrimental to defendants with appointed counsels in opposition to retained attorneys. Still, a round about 18% higher conviction rate is remarkable, as well as 14% lower rate of dismissed cases and something that surely deserves further inquiry. Similarly, indigent defendants face a higher sentence and on average higher fines. Note that indigence is defined upon net income. We need to remark though, that *Convicted* is indeed a summary. It consists of all guilty pleas, nolo contenderes and actual convictions at trial. 

Table 2 suggested, that there is a disparity in the experience between the two lawyer groups. Since we haven't looked at a chart yet, let's investigate further into experience inequalities and learn something about visualisation in R. The aim of the next tasks, is so replicate the results in *figure 1 A* of the authors paper.


**Task:**
The variable *experience* indicates the years in practice since attorney bar admission. A `summary()` shows, that the values range from -3 to 41. Define a second variable *exp.fig* and add it to the data, that recodes all values below zero as `NA` and all values above 35 as 35. 
Use the `ifelse()` function and the intermediate variable *exp.int* to accomplish the task. 

```{r "1_2__6"}
# In the proximate line the first step is already stated, just remove the '#'.
# exp.int <- ifelse(Lawyers$experience>=0, Lawyers$experience, NA)
# Now code the second line, using ifelse() and exp.int

```


The variable *exp.fig* will be the x variable in our plot. But we are still necessitate a y variable. Unfortunately, this one isn't so easy to determine, but stick with me and your will be rewarded with a nice plot. The approach is first to group by the attorneys and their experience to compute the mean appointed rate and than to group by the experience only and compute the overall fraction of cases assigned per year of experience. You don't have to do the computations yourself until step 4, so just press `check` in the prior steps.

**Task - Step 1:**
Compute the variable *pct.appt.exp* as the mean of appointed cases grouped by attorney and experience.
Additionally compute *n.atty.exp*, which incrementally counts the number of observations in each group.

```{r "1_2__7"}
Lawyers <-  Lawyers %>% group_by(attybarcard,exp.fig) %>% mutate(pct.appt.exp=mean(appointed), n.atty.exp=row_number()) %>% ungroup()
```

**Task - Step 2:**
This is just an auxillary step. Generally, an attorney looks after many cases during a year, which guarantees us, that we have indeed multiple observations when grouping by attorney AND experience. This is essential to compute the mean in the preceding step. However, `mutate()` adds this value to every case in our data, not just on a group level, although we need it just once per group to carry on. We therefore compute the intermediate variable **int.appt.exp**, which uses *n.atty.exp* to set just the first value in each group to the value of *pct.appt.exp* and all other to `NA`. 

```{r "1_2__8"}
Lawyers$pct2 <- ifelse(Lawyers$n.atty.exp==1, Lawyers$pct.appt.exp, NA)
```

**Task - Step 3:**
In this step we finally group and sort by *exp.fig* to compute the overall *appointed* mean - *mean_pct*.

```{r "1_2__9"}
Lawyers <- Lawyers %>% arrange(exp.fig) %>% group_by(exp.fig) %>% mutate(mean_pct=mean(pct2,na.rm=T)) %>% ungroup()
```

**Task - Step 4:**
Finally we will plot the graph. We exploit the `ggplot2` package to do so. All you need to know about it is, that `ggplot` operates layer-based, with the first layer specifying the data and x plus y variable. Replenish the code in the function call `ggplot()` with our, in steps one and three, determined x and y variable.
**Note:** Take a look at the info box below for more information on `ggplot()`.

```{r "1_2__10"}
ggplot(data = Lawyers, aes(x = ___, y = ___)) + geom_line(colour = "skyblue4", size = 1.25) + theme_bw() + labs(title = "Appointment # Rate per Experience", y = "% of assigned Cases") + theme(plot.title = element_text(hjust = 0.5))
```

info("ggplot()") # Run this line (Strg-Enter) to show info

As the figure shows the share of assigned cases declines with ascending years of experience. Especially between five and ten years of experience their seems to be a slump. This corresponds to the findings in table 2, "attorney characteristics". 
By this point we could suggest that the worse case outcomes we looked at table 3 could be explained by the lower experience the assigned attorneys have on average. However, we neither know yet whether this hypothesis holds, nor do we know the reasons why lower experienced counsels preferential advocate assigned cases. So let's jump to the next **Exercise** and run our first model to get a deeper understanding of the topic.

<div class="garbage">

info("ignore this, this is just a garbage collector for the tables cube") # Run this line (Strg-Enter) to show info
</div>


## Exercise 2 -- Investigate Lawyer Heterogeneity

After we get some insights into the fundamentals of R as well as the paper we will now continue to dive deeper into both. In exercise 2.1 we will run our first regression analysis. If you are already familiar with OLS that shouldn't be something, that scares you. If you are not, you will learn the basics in the exercise. Additionally, after you finished exercise 2.1 you will now, what the authors mean, when they talk about the (unconditional) assigned counsel penalty and how to estimate it. In exercise 2.2 we will focus entirely on the theory of panel data models. You will encounter error component models and fixed effects estimators in this exercise, but once you understand the concepts explained there, you will realize, that it isn't a big of a deal. Finally we will utilze our newly required knowledge about panel data estimation methods to assess the conditional assigned counsel penalty, which is the basis for further analysis, that the authors undertool and we will too in exercise 3.

## Exercise 2.1 -- Unconditional Assigned Counsel Penalty

**Task:**
Press `check` to read the data.

```{r "2_1"}
Lawyers <- readRDS("Lawyers.rds")
```


In order to be able to deal with the disparites in case outcomes between indigent and non-indigent defendants appropirately, it is crucial to designate the underlying source of disparity. The authors cite four possible reasons for the blatant differences: 

<ol style="list-style-type: upper-roman">
  <li> <b> Case Characteristics: </b> it may be harder to defend indigent clients compared to non-indigent clients </li>
  <li> <b> Adverse Selection: </b> counsels who register to serve as assigned attorney may be less proficient </li>
  <li> <b> Matching Mechanism: </b> the assignment approach could be adverse for indigent defendants compared with the endogenous process in the privat market </li>
  <li> <b> Moral Hazard: </b> attorneys of indigent clients might invest less effort into their cases, relative to cases in which they are paid directly by their clients </li>
</ol>

In most related literature disparity is mainly explicable due to case characteristics and adverse selction. As an example, *Roach (2014)* examines how the assigned counsel outcome changes based on the outside option relative to public counsels. The outside option is defined upon the privat market wage destribution. Instead of utilising the whole distribution, he employs the median for high-quality attorneys and the ten-percent quantile for low-quality attorneys. He finds that assigned counsel outcomes improve relative to public defender outcomes if the low quality type's outside option rises or if the high quality type's outside option declines. In detail, *Roachs (2014)* main specification yields that a one-dollar rise in the outside options of high quality attorneys increase the expected sentence length by approximately half a month and a one dollar increase in the low quality counsels outside option lowers expected sentence by around 0.9 months. Both results are significant on the 10% and 5% level and reveal a plain sign for adverse selection. 

In *Abrams and Yoon (2007)* the researchers scrutinized whether there is heterogenity in case outcomes across public defenders and subsequently investigated correlations between attorney characteristics and case outcomes. That is, their study focuses on differences in attorney quality within the group of public counsels. Unfortunately the study doesn't incorporate a comparison to assigned or private attorneys. Therefore we can't speak of adverse selection (between two groups) as defined in the listing above. 

Conversely, *Iyengar (2007)* overcomes this minor shortcoming by assessing the differences between public counsels and CJA panel attorneys, similar to *Roach (2014)*. However, while *Roach (2014)* examines data comprising cases in state courts, *Iyengar (2007)* investigates federal criminal cases. By adducing attorney experience, wages, law school quality and average caseload *Iyengar (2007)* is able to explain half of the overall difference in performace. 

By now, the question arises how well our data renders possible to gauge differences in attorney performance. Recall that, unlike *Iyengar (2007)*, we are primarily interested in differences between appointed and retained attorneys. So, let's undertake the requisite steps to answer that question. 

We start by simplay regressing our desired outcome variable $guilty$ on $appointed$, 

$$\begin{align}
guilty_{i} = \beta_{0} + \beta_{1}*appointed_{i} + \epsilon_{i}, \qquad \text{(1)}
\end{align}$$

where $i$ denotes one of 64209 cases and assuming $\epsilon$ fulfills 

$$\begin{align}
\mathbf{E}[\epsilon|appointed]=\mathbf{E}[\epsilon]=0. \qquad \text{(2)}
\end{align}$$

In accordance with the terminology from the paper, we call the estimate $\hat{\beta}$ of the $\beta$ in this basic model the **uncoditional assigned counsel penalty**. 
The assumption we made here is one of the five Gauss-Markov Assumptions that constitues unbiasedness and homogenity of OLS. We are assuming the other GM-Properties to hold for now. If you feel the need to revise the basics of OLS take e.g. a look at *Wooldridge (2013)*.
If you are familiar with the basics of OLS and R you know that we can run this model in R by using the `lm()` function. 
However, we can achieve identical results by applying the `felm()` function from the `lfe` package.

**Task:** 
Use the `felm()` function to run the regression model. 
The solution is already provided, so just press `run chunk`.

```{r "2_1__2"}
model1 <- felm(guilty ~ appointed, data = Lawyers)
summary(model1)
```

---

Take a look at the regression output and try to answer the quiz questions afterwards.

#! addon__quiz__magnitude of appointed 1

---

#! addon__quiz__significance 1

---

#! addon__quiz__conviction rate PD

---


I recommend you take a look at this note to get a notion of the interconnections. Alternatively, skip the note and let's proceed with an more elaborate model and justifying the use of `felm()`.

#! start_note "Why we don't need the felm()-function to obtain this results"

Aforementioned, I already anticipated that we don't rely on `felm()` at this point.

**Task:**
For Expediency, run the same regression with the `lm()` function and assure yourself that they both yield the same results. Don't forget to use `summary()` to produce an output of the regression.

```{r "2_1__3",optional=TRUE}
# Enter your code here.
```

Obviously, we get the same results.
Note, how the solution from the regression exactly matches the values in table 3, "case outcomes", of our previous exercise. This is not a coincidence. If we regress one binary variable on another we get two fitted lines. Both are horizontal, with the first line equal to $\beta_{0}$, which in turn is the same as the mean of convicted outcomes among public defenders and the second line equals $\beta_{0} + \beta_{1}$, which again is alike to the assigned counsel conviction rate. Basically, it all boils down to comparing two means with each other, and assessing the significance. From statistics 101 you should know, that the t-test is designed to accomplish this task. 

**Task:**
The solution is already filled in. Click on `check` to run the t-test.

```{r "2_1__4",optional=TRUE}
t.test(formula = guilty~appointed, var.equal=T, data = Lawyers)
```

#! end_note

At this point it seems like we are finished with our analysis. We constructed an econometric model and tested it empirically with significant results, so what is left to do? Well, maybe we should start with revisiting the unbiasedness assumption $\mathbf{E}[\epsilon|appointed]=0.$ and in this context discuss the actual meaning of "significance". You can take a look at the respective info box for a more elaborate explenation. But basically, you already have a grasp of significance, if you solved the second quiz question before and therefore let's focus on unbiasedness. The unbiasedness assumption is also called the "zero conditional mean" assumption and states that $\epsilon$ is uncorrelated with $X$, in our case $appointed$. If we include only $appointed$ in our model you should be sceptic if this assumption holds, as $\epsilon$ comprises all other variables, that we obsered but didn't include into our model. As an example, solce the adjacent task.

**Task:**
Use the `cov()` function to compute the covariance between $appointed$ and $experience$.

```{r "2_1__5"}
# Enter your code here.
```

As you see both variables are correlated. Hence, omitting $experience$ would lead to correlation between $appointed$ and $\epsilon$, which thereupon would violate our assumption - this is called **omitted variable bias**. Without **zero conditional mean** assumption in place we get biased estimates and thus loss causality. 


info("Unbiasedness and Significance") # Run this line (Strg-Enter) to show info

Fortunately, if we observe many potential factors, as in or case, we can solve the omitted variable problem and validate the critical assumption. So, let's run a second regression with a richer set of explenatory (and hopefully exogenous) variables.

**Task:**
Feel free to simply click on `check` to run the model.
Again, using `felm()` or `lm()` is arbitrary.

```{r "2_1__6",results='asis'}
model2 <- felm(guilty ~ appointed + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + caseload_retained, data=Lawyers)

summary(model2)
```


Please look at the regression output again and then try to solve the quiz.

---

#! addon__quiz__magnitude of appointed 2

---

#! addon__quiz__significance 2

---

#! addon__quiz__economic significance

---

#! addon__quiz__background info

---

Although the outputs produced by `felm()` and `lm()` are identical in terms of the coefficients and all related statistics, there are differences in the reporting of $R^2$ and $F$ statistics. While `lm()` reports each once, `felm()` reports each twice. This is linked with the motivation to enter the next exercise: With adding case and attorney controls we already improved the first basic model but as for now we haven't really took adventage of our dataset structure. Proceed to the next exercise, if you want to continue. 


## Exercise 2.2 -- Panel Data and Causality

**Task:**
Press `check` to read in the data.

```{r "2_2"}
# First of all we read in the data again.
Lawyers <- readRDS("Lawyers.rds")
```


So far we have treated each observation as independent, which is the characteristic thing to do in a cross-sectional data analysis. But in our case we have the virtue of panel data (or longitudinal data as it is also called). The feature of panel data is adding a time series dimension to the cross-sectional units. In our circumstance, this means we observe the same attorney over several years (remember we have data for the period 2005-2014). We distinguish between balanced and unbalanced panels. If we miss out to observe attorneys in some years, the panel is called unbalanced. Conversely, if we observe all attorneys in all years the panel is balanced. 

**Task:**
Determine whether our panel is balanced.
The code is already supplied, just press `check`, look at the output and answer the quiz question afterwards.

```{r "2_2__2"}
# Use dplyr once more to retrieve the information in demand
balanced <- Lawyers %>% 
  group_by(attybarcard) %>% 
  summarize(number_of_obs=n()) 

kable(balanced) %>% kable_styling("striped", "hover", "condensed", position= "left", full_width = F) %>% scroll_box(height = "300px")
```

---

#! addon__quiz__Balanced and Unbalanced Panels

---

Note, albeit our panel in unbalanced, we will derive all the theory, that is stated in this exercise based on the assumption of balanced panels. The reason to do so is, that I want to prevent you from feeling intimidated or overwhelmed by complexity, that isn't much contributing to understand the fundamentals of this problem set. Thus, at the end of this exercise you can find a caveat (among others), that explains the manner in unbalanced panels. 

Now that we have discussed some terminology, let's ask the important question - What are the benefits compared to cross-sectional data?

- **Individual Heterogeneity:** Panel Data offers the possibility to control for individual heterogeneity whereas cross-sectional and time-series data doesn't render this opportunity. Let's explain this in plain language in relation to our dataset. With cross-sectional data we obtain our results, because there is variation in attorneys characteristics. With panel data we are able to observe the same attorneys across a time dimension and hence observe some characteristics that vary over time and some that don't, e.g. $caseload_retained$ is time-variant whereas *innate ability* is time-invariant. We can therefore control for time-invariant variables like *innate ability* and draw results from variation within each attorney. If you take a look at our dataset, you will recognize that there actually isn't a variable called *innate ability*. The reason is that we can't observe it. Hence we call it **unobserved heterogeneity** and panel data models allow us to control for it. If we don't account for unobserved heterogeneity we run into the risk of misspecification and eventually biased results.
- **Less Collinearity:** In opposition to time-series data, which often suffers from high multicollinearity, panel data benefits from more variability through adding a cross-sectional component.
- **Dynamics of Adjustment:** Cross-sections can tell us something about the effect of being assigned on conviction at one point in time. Pooled Cross-sections deliver information on how this relationship shifts over time. But only panel data enables us to conclude how assignment influences conviction in the cases where one attorney is appointed as opposed to the cases where the same attorney is retained.
- **Identification and Measurement:** We determined already, that the conviction rate among assigned attorneys is 8.3% higher compared to retained counsels (after including controls and by pooling cross-sections). But we can't use the same approach to answer the following: Do all attorneys perform 8.3% worse (in terms of conviction) if they are assigned or rather, do 50% of the attorneys perform identically no matter of assignment while the other 50% yield 16.6% worse results. Anwering this question is essential if we want to denominate the reasons for outcome differences (as stated at the beginning of Exercise 2.1).  

info("Unobsered Heterogenity and Endogenity") # Run this line (Strg-Enter) to show info

Great, now we now how panel data is established and about the advantages it exhibits. The next logical step is thence to fathom the model. But before we draw our attention to the model and the estimation method used by the authors let's first lay down the theoretical baseline model. 
We can describe a panel data model by
$$ \begin{aligned} 
  y_{it} = \alpha + X^´_{it}\beta + \epsilon_{it} \quad i = 1,...N; \quad t = 1,...,T, \qquad \text{(4)}
\end{aligned} $$
where $y_{it}$ is the outcome/response variable, $\alpha$ is the intercept, $X^´_{it}$ is the set of explenatory variables, without fixed effects and $\beta$ being a vector of slopes. 

Panel data applications utilize **error component models** to split up the error term $\epsilon_{it}$:
$$ \begin{aligned} 
  \epsilon_{it} = \mu_i + u_{it} \qquad \text{(5)}
\end{aligned} $$
While $u_{it}$ represents the disturbance, the *unobservable individual-specific effect* is denoted by $\mu_i$. In our circumstance the individual or unit is the respective attorney. Note that $\mu_i$ is time-invariant and accounts for any attorney-specific effect that is not included in the ordinary regression. In fact, we already discussed one possible attorney-specific effect, namely *innate ability*, but of course we can think of many more effects, like *sense of justice*. There are different methods to estimate the model, which all possess different assumptions. Depending whether the respective assumptions are fulfilled, the estimation models differ in unbiasedness, efficiency and consistency. Since this isn't a course on mathematical statistics, we won't discuss all of them. If you are interested in the mathematical details, you can chech out *Hsiao* (2014) or *Baltagi* (2014). Nevertheless, we have to pick one model, and we will use the **fixed effects model**. (Though, I don't want to miss at least mentioning the **random effects model**, which is another prominent framework to derive unbiased coefficients in error component models.)
First let's make clear, why we can't directly apply OLS to the error component model. Therefore, we again take a look at the critical unbiasedness assumption stated in equation (2):

$$\begin{align}
\mathbf{E}[\epsilon_{it}] = 
\mathbf{E}[\mu_i + u_{it}] = 
\mathbf{E}[\mu_i] + \mathbf{E}[u_{it}] = 
\mathbf{E}\left[
  \begin{pmatrix}
      \mu_1  \\\
      \mu_2  \\\
      \vdots \\\
      \mu_N  \\\
  \end{pmatrix}\right] = 
\begin{pmatrix}
      \mathbf{E}[\mu_1]  \\\
      \mathbf{E}[\mu_2]  \\\
      \vdots \\\
      \mathbf{E}[\mu_N]  \\\
\end{pmatrix} =
  \begin{pmatrix}
      \mu_1  \\\
      \mu_2  \\\
      \vdots \\\
      \mu_N  \\\
  \end{pmatrix} =
\mu_i \neq 0. \qquad \text{(6)}
\end{align}$$

The second equality follows from $u_{it} \sim {iid(0, \sigma_u^2I_T)}$. So now as we know that we can't apply OLS to the model, we need to come up with a new set of assumptions in order to derive unbiased estimates. We do so by invoking:
$$\begin{align}
   \mathbf{E}[\epsilon_{it}|X_{i}\mu_i]=0. \qquad \text{(7)}
\end{align}$$
This assumption is called **strict exogenity**. Note that this assumption only prohibits correlation between $\epsilon_{it}$ and $X_{it}$/$\mu_i$ respectively. However, correlation between $X_{it]$ and $\mu_i$ is permited. Note that it isn't the only assumption we need to impose, but it is definitely vital. Another not negligible assumption is that each explenatory variable changes over time, and no perfect linear relationship exists among the explenatory variables. Further, with the error component model we already established **linearity** (of the coefficients). You can take a look at the remained set of assumptions in *Wooldridge* (2015).
Basically, strict exogenity is vital, because it permits to get unbiased and for fixed $T$ as $N \rightarrow \infty$ even consistent estimates of $\beta$. It also satisfy the consistency property, if $N$ is fixed and $T \rightarrow \infty$ or, if both (individuals and time periods) aspire infinity. While consistency is a beneficial property, here, we really want to stress unbiasedness. (Check out the info box, if you are not familiar with the consistency property.)

info("Consistency") # Run this line (Strg-Enter) to show info

In the last exercise we already explained what unbiasedness means. Again, we need to remember, that our dataset is just a random sample of cross sections across time from a population. If our estimator is biased the relationship between  $y_{it}$ (in our case $guilty$) and $X_{it}$ (in our case $appointed$) differs from the true relationship in the population. We already mentioned, that biasedness therefore also prohibits statements about causality. Though, the interesting question is, whether we can draw causal statements from our model even if we obtain unbiased estimates. Let's follow up on that by simply asking, what is causality? Causality is the link between a cause and an effect across a time dimension. Well, that doesn't help much. Indeed, there is a framework which was devised by *Rubin* (1973a, 1973b, 1974, 1977) upon the work of *Nejman* (1923) and refined by many statisticians like *Holland* (1986) and *Rosenbaum* (1984). It is called the **Rubin Causal Model (RCM)**. However, we will present the key findings of RCM based on *Morgan, S. L. & Winship, C.* (2015). 
Accordingly, we can define causal states $D$ and potential outcomes $Y$. We will only deal with binary causal states here, since our research question focuses on the binary variable/cause $appointed$ with the two causal states *assigned*, if $appointed=1$ and *retained*, if $appointed=0$. In the case of binary causal states we can also call them **treatment** (corresponding to *assigned*) and **control** (corresponding to *retained*). In this case we can then define the individual-level causal effect of the treatment as:

$$\begin{align}
  \tau_i=y_i^1-y_i^0, \qquad \text{(8)}
\end{align}$$
where $y_i^1$ and  $y_i^0$ are realisations of the potential outcome in the treatment and control state for individual i. Since the potential outcome is a random variable we can denote it as:

$$\begin{align}
  Y=DY^1+(1-D)Y^0. \qquad \text{(9)}
\end{align}$$
Equation (9) implies that one can never observe both causal states simultaneously and hence equation (8) is impossible to calculate. This is called the *fundamental problem of causal inference*. The unobservable potential outcome is called *counterfactual*. 
By now, slowly the notion could arise, why estimating even an unbiased coefficient could be not enough to obtain causality. Ideally we would like not only to observe the outcome ($guilty$) of an attorney in the case where he is *assigned*, but also **in the same case**, where he is *retained*. Again, that's of course counterfactual, but let's see how we can proceed from here. 
The solution is to calculate the average across the individual-level causal effect, which is called **average treatment effect (ATE)**: 

$$\begin{align}
 \mathbb{E}[\tau]=\mathbb{E}[Y^1-Y^0]=\mathbb{E}[Y^1]-\mathbb{E}[Y^0]. \qquad \text{(10)}
\end{align}$$

You should note two things. Firstly, that ATE is defined on the population (and not on the random sample) level. Secondly, that there isn't just one ATE, which we can apply to all models but rather many different ATEs. As an example, you can find the conditional ATE in the info box attached. 

info("Conditional ATE") # Run this line (Strg-Enter) to show info

At this point we should introduce an important assumption, which is inflicted on RCM models - the **Stable Unit Treatment Value Assumption (SUTVA)**. In plain words, it states that individual potential outcomes are independant of the treatment exposures of other individuals. If SUTVA doesn't hold, equation (8) doesn't either and concequently the ATE effect is biased. 

---

#! addon__quiz__SUTVA

---

In the case of randomized experiments the treatment statuts is independent of the potential outcomes. We call this the **ignorability** of the **treatment assignment mechanism** and state it as:
$$\begin{align}
 (Y^0,Y^1) \perp D. \qquad \text{(11)}
\end{align}$$
Indeed, ignorability is even preserved, if the potential outcomes are independent of D, condition on S. Learning the treatment to which a subject has been exposed gives no information whatsoever about the size of the treatment effect for a properly run randomized experiment. This means, in randomized experiments ATE from equation (9) can be used to infer causality. 
The thing is, in our circumstance we don't have a randomized experiment and therefore no control over the treatment assignment mechanism. All we have are observations and we can't assume ignorability to hold in this case. Instead of assignment we need to investigate how individuals (attorneys) are selected into alternative treatment states, that is to say, investigate the **treatment** **selection** **mechanism**. This is the reason, you often find extensive discussion about how and who was selected. If you read an observational study without discussion of treatment selection mechanisms, you should be sceptical about the validity of the results of this study. For example have a look at a quote from *Roach* (2014), discussing the treatment selection mechanism: 
> "This analysis focuses on counties that use both public defenders and
assigned counsel to represent indigent defendants, and so the institutional
details surrounding the mechanism by which cases are assigned between
these groups is important, particularly as it relates to measuring differences
in outcomes between these groups. These details inform the plausibility
of assuming random assignment between the two groups as well as any
caveats to the results. While these assignment mechanisms certainly differ
between counties in some respects, a county’s mechanism for assigning
cases between these two groups has several important features: how the
set of attorneys eligible for assigned counsel work is determined, rules for
when assigned counsel are to be used, the rules by which that assignment
decision is made, and the entity that actually assigns counsel to a defendant."


Are you confused, why he uses the term "assignment" in an observational study, instead of "selection"? The reason is that, basically, in all court districts the court, judge or another official assigns the CJA panel attorneys to an indigent accused. This means from the perspective of the court/judge/official, it is a treatment assignment mechanism, since they are the one who control treatment assignment. But from the perspective of the researcher it is treatment selection mechanism, because they do not have the control over it. Indeed, the treatment selection mechanism is nested, because firstly the attorney decides to enroll in the panel and secondly gets assigned by court/judge/official. 

Either way, in our circumstance, in Bexar county, a third party, with no specific information about the case or lawyer is appointing attorneys to indigent accused from a pool of lawyers who have registered with the county. Counsels assigned to defendants can't turn the case down, and defendants can't select among attorneys conversely. All attorneys with at least one year of experience in criminal law are eligible for the Bexar county assigned counsel system. The defendant gets interviewed by a pre-trial service clerk to identify which requirements the attorney must match. Such requirements are: language, degree of offense, availability of attorney and date of last appointment. The latter is important, because out of all eligible attorneys on the list, the one with oldest date of last appointment gets assigned. This is done to assure a system of rotation. Besides that, we should definitely keep this matching characteristics in mind. They have the potential to highly impact the results in terms of causality. For example, suppose speaking a certain language is causal interacted with harder to defend crimes. This would critically affect the disparity in case outcomes. You can have a look at the [standards and Procedures related to appointment of counsel for indigent defendants](http://tidc.tamu.edu/IDPlan/ViewPlan.aspx?PlanID=26) for additional details on the assignment mechanism. Overall, there is one important thing to consider: Defendants who are technically eligible for indigent defense may choose to borrow money to retain private counsel if they expect that the return of doing so is high. This leads to **selection bias**.  
We can define selection bias, utilizing RCM, as 
$$\begin{alignat}{3}
  \mathbb{E}[Y_i|D_i=1]-\mathbb{E}[Y_i|D_i=0]&=\mathbb{E}[Y_i^1-Y_i^0|D_i=1]+(\mathbb{E}[Y_i^0|D_i=1]-\mathbb{E}[Y_i^0|D_i=0]) \\\
  &= \quad ATE \quad + \quad Selection \quad Bias. \qquad \text{(12)}
\end{alignat}$$
Note that we call the expression on the left hand side the **observed effect**. Moreover, you should pay close attention to the $Y$ values on the left hand side - we dropped the upper index. We did this because the left hand side of equation (12) states the actual outcomes, which we observe by drawing a random sample, while the right hand side of the equation still preserves the upper index and therefore represents the population. Also note, that we included subscripts to refer to individual units across which we calculate the averages, instead of the random variables before.  
If the treatment assignment mechanism is not random, we run into the risk not only to observe the ATE. This happens if the selection bias is different from zero. That is always the case if the potential outcomes $Y_i^0$ from those that are treated $D_i=1$ differ from the potential outcomes $Y_i^0$ from those that are not treated $D_i=0$. Of course the sign of selection bias could either be positive or negative. 

After we discussed the differences between experimental and observational data, let's go back to the ATE. So far, we only stated the basic framework. However, now we need to extend the formulas to satisfy our panel data. Therefore we restate eqaution (9), such that
$$\begin{align}
  Y_{it} = D_{it}Y_{it}^1+(1-D_{it})Y_{it}^0, \qquad \text{(13)}
\end{align}$$
where $Y_{it}$ is the potential outcome of individual $i$ at time $t$ and $D_{it}$ is a time-varying variable, indicating whether individual $i$ receives (or selects) treatment in time period $t$. $D_{it}$ remains equal to zero over time for all members of the control group but varies between 0 and 1 for all members of the treatment group. We can define a time-constant  treatment group indicator $D_i^*$, that indicates wheter individual $i$ ever receives (or selects) the treatment at any point in time during the observation window of the study. In cross-sectional analysis, where all observations occur at a single time period, it holds that $D_{it}= D_i^*$. In panel data, however, $D_{it}$ and $D_i^*$ can differ and we can exploit the differences to estimate causal effects. And how can we finally achieve this? 

Remember that ATT is defined on the population level. Therefore it would be favourable, if we could infer from our sample an estimator $\hat{\tau}$ of the population ATT in order to obtain causal results. Now, here is the good message, we have everything in place by now to do so. If our fixed effects model satisfy the strict exogenity assumption, we provided in equation (7) and additionally is linear, than according to *Imai, Kim* (2019) the fixed effects estimator is equal to
$$\begin{align}
 \tau_{FE}=\mathbb{E}[Y_{it}^1-Y_{it}^0|C_i=1], \quad \text{where} \qquad \text{(14)}\\\
 C_i \iff \mathbb{1}\{0<\sum_{t=1}^T D_{it} < T\}. \qquad \text{(15)}
\end{align}$$

This means, if we now that our fixed effects assumptions hold, and we thereofore obtain an unbiased estimator, we silmutaneously can be assured to derive causal results. Well, if that isn't great, I don't know what is. 
Finally, note that $C_i$ comprises the identification condition. That is, there must be variation in the treatment variable $D_{it}$ across time $t$. If for an individual $i$ treatment is never assigned (or selected) throughout the whole time period $T$, the condition is violated. The same applies, if for an individual $i$ treatment is assigned (or selected) for all $t\in T$. You can have a look at *Imai, Kim* (2019) if you want to obtain an even profounder insight into the causality of fixed effects models beyond the scope of this problem set. 


The last remaining thing to do in this exercise before we go on is to explicitly denote the fixed effects estimator, which is also called the **Within Estimator** or **Least Sqaures Dummy Variable (LSDV) Estimator**. There are different approaches in order to calculate it, which we won't cover here. Instead, in this exercise, we will provide the estimation in the case of $X_{it}^´$ from equation (4) being a single explenatory variable, which we hence denote as ${x_{it}}^´$. Note that ${x_{it}}^´$ equals $appointed$ in our circumstance.

In fact, we could solve the problem described in equation (6) by including $N$ dummy variables for the unobservable individual-specific effect stated in eqution (5) and then apply OLS as estimation method, this is than called a LSDV regression (which explains the alternate name of the fixed effects estimator). The problem in this procedure arises from the fact, that the dummy variables highly increase the dimension of the design matrix $X_it$, which encompasses the treatment variable and all fixed effects.

---

#! addon__quiz__dimensions

---

This circumstance means that $X_{it}$ is quiet big and therefore inconvenient to inverse in order to get the estimator. Consequently, we need a way to consider the individual heterogenity without overloading the degress of freedom. The solution is to average out (demean) the fixed effects. Applied to equation (4), we get

$$\begin{align}
  y_{it}-\bar{y_{i.}}=\beta (x^´_{it}-\bar{x^´_{i.}})+(u_{it}-\bar{u_{i.}}), \qquad \text{(16)}
\end{align}$$
where $\bar{y_{i.}}=\sum_{t=0}^Ty_{it}$, $\quad \bar{x^´_{i.}}=\sum_{t=0}^Tx^´_{it} \quad$ and $\quad \bar{u_{i.}}=\sum_{t=0}^Tu_{it}$. 

Because the fixed effects are time-invariant they drop out of the equation, which leads to our desired dimension reduction. Unfortunately, we lose all variables, who are constant across time. The same applies to the intercept, because the corresponding column in the design matrix is obviously a vector of 1s. However, we can retrieve the estimates of the intercept and the fixed effects through some additional computations, as stated beneath. Note that we need to utilize the restriction 
$$\begin{align}
  \sum_{i=1}^N\mu_i=0, \qquad \text{(17)}
\end{align}$$
in order to avoid the dummy variable trap (perfect multicollinearity), which would render the inversion of the design matrix impossible. Besides that, equation (15) changes the interpretation of the fixed effects estimates and the intercept. Without the restriction in place, running LSDV regression would produce estimates for $\beta$ and $(\alpha+\mu_i)$ and not for $\alpha$ and $\mu_i$ seperately. With the restriction in place we can recover the respective estimates, individually. The info box "Separate Estimates" explains, how to do that. Note, that the estimate for $\beta$ is the same, with and without the restriction imposed. 

info("Separate Estimates") # Run this line (Strg-Enter) to show info

So let's finally state the fixed effect estimator in closed form (for the case of a single regressor in $X_{it}^´$).
In the tradition of OLS, we want to minimize the sum of sqaured errors in regression (14):
$$\begin{align}
  \hat\beta_{FE} = \underset{\beta}{\operatorname{argmin}}{\sum_{i=1}^N\sum_{t=1}^T[(y_{it}-\bar{y_{i.}})-\beta(x_{it}^´-\bar{x_{i.}}^´)]^2}. \qquad \text{(18)}
\end{align}$$
Solving this minimization problem yields the estimator:
$$\begin{align}
  \hat\beta_{FE} = \frac{\sum_{i=1}^N\sum_{t=1}^T(x_{it}^´-\bar{x_{i.}}^´)(y_{it}-\bar{y_{i.}})}{\sum_{i=1}^N\sum_{t=1}^T(x_{it}^´-\bar{x_{i.}}^´)(x_{it}^´-\bar{x_{i.}}^´)^T}. \qquad \text{(19)}
\end{align}$$

Note that the fixed effects estimator is the **best linear unbiased estimator (BLUE)**, if all assumptions we stated so far are fulfilled. An estimator is called BLUE among a set of estimators, if it has the lowest variance. We already highlighted, that as long $T$ or $N$ approach infinity, $\hat\beta$ is consistent. However, the intercept is only consistent for $T \rightarrow \infty$ but always unbiased.

Fortunately, we don't have to compute the estimate by hand, since the already introduced `felm()` function from `lfe` package performs this task for us. Anyway, it's crucial to know the theory, in order to assess the output R provides to us. However, what we need to know is the syntax of the `felm()` function, especially of the first argument, *formula*. The *formula* comprises four pars. The first part contains the ordinary covariates, the second part encompasses the fixed effects to be projected out, the third part allows for insturmental variables and the last part adds the option to specify clustered standard errors. Please inspect the help page, for an example of the *formula* syntax and for further information, particular about additional arguments. The `lfe` package was developed by *Gaure* (2013a, 2013b). You can have a look at his enclosed papers for some clearification of fixed effects estimation with `lfe`. 

We have to discuss some important caveats before proceeding:

1. **Unbalanced Panel:** As already mentioned, all theory about causality and estimator properties stated in this exercise applies technically only to balanced panels. Yet, we have concluded that our panel is unbalanced. Well, this doesn't do much harm to us, according to *Wooldridge* (2015), at least from a computational viewpoint. All we have to do is to replace the overall time periods $T$ with the individual total time periods $T_i$. Consequently, the total number of obsertations amounts to $T_1, ..., T_N$. However, if possible the reason for missing data should always be investigated, since it can harm our assumptions about the error term and subsequently distort causality. Although not needed in this problem set, you might want to have a look at chapter eleven of *Hsiao* (2014). 
2. **Error Components:** The error component model also allows to parse the error term beyond individual-specific heterogenity by including a fixed effect capturing unit-invariant heterogenity across time. We won't apply such models here, but a great source for further study is *Baltagi* (2014). Although, we won't use a model with more than two error components, we will necessitate a estimation procedure handling three-way fixed effects. It would be to cumbersome to discuss such approaches in detail, compared to the benefits of solving this problem set. However, if you are interested in it, have a look at *Somaini, Wolak* (2016) for estimation of two-way fixed effects models and at *Balazsi, Matyas, Wansbeek* (2018) for an general estimation approach of multidimensional fixed effects models.
3. **Model and Data Structure:** In our theoretical explenations we always considered a data structure, where we have time periods $t$ nested within individuals $i$. However, our data adds a level to this hierarchical structure. We have attorneys $k$, which we observe across various time periods $t$ (so far, so good), but we observe multiple cases $i$ handled by the attorney within a year. We will discuss the implications of this differences in the next exercise, when we will explicitly state the model used by the authors. While this is a crucial difference, you can direcly apply the knowlege, which you acquired throughout this exercise. 



Click `Go to next exercise...` to proceed.



## Exercise 2.3 -- Assigned Counsel Penalty

First, let's read in the data again, we will need it.

**Task:** The code is already supplied, just press `check`. 
```{r "2_3"}
Lawyers <- readRDS("Lawyers.rds")

```

In this exercise we will apply the findings from the previous exercise in order to determine the **conditional assigned counsel penalty**. We already examined the unconditional assigned counsel penalty in exercise 2.1 and introduced some covariates, that could potentially help in explaining some of the unconditional penalty. We will use the same set of regressors again, but with the difference of exploiting the features of panel data, instead of pooled cross-sections. Note that we will from now on drop the "conditional"" and simply refer to **assigned counsel penalty**. Again, we will conduct our analysis based on the outcome variable $guilty$ and provide the results for other outcomes subsequently. 

Based on the paper, the model we want to estimate is 

$$\begin{alignat}{3}
  y_{ikt}&=\delta + \beta_1assigned_{ik}+X_{it}\Omega+A_{ikt}\Pi+\epsilon_{ikt}, \qquad \text{(20)} \\\
  where \quad X_{it}&=[D_i, o_i, c_{it}] \quad and \quad A_{ikt}=[a_{ik}, \gamma_{kt}]. 
\end{alignat}$$

Hereby, we can estimate $\beta_1$ which is the assigned counsel penalty and $y_{ikt}$ being the response variable. The matrixes $X_{it}$ and $A_{ikt}$ comprise defendant/case characteristics and attorney characteristics. Primarily, this are the same covariates, that we already introduced in exercise 2.1, but with additional breakdown into several components. For instance, $X_{it}$ is subdivided into $D_{it}$, $o_i$ and $c_{it}$. Note that, $D_{it}$ contains the already familiar defendant variables $male$, $hisp$, $white$, $age\_offense$, $povrate$, $instability$, $hadbondsman$, $complaint\_hist$ and $convict\_hist$.   
Case characteristics $o_i$ and $c_{it}$ are captured by the two categorical variables $offense\_num$ and $court\_docket$. With $offense\_num$ we capture one of 659 offense codes, which distinctly describe the offense. Originally, the offense codes are 8 digits long, where the first four digits depict the NCIC Classification (not important here) and the last four digits represent the Texas specific identifier of the offense. If you have a look at our data, you will see, that we only have four digit offense codes. This is, because our data only provides the Texas specific identifier. For instance, the code "1084" represents interfering with public duties. You can have a look at [Texas Department of Public Safety](https://www.dps.texas.gov/administration/crime_records/pages/appndxkoffensecodes.htm) for additional information.    
The variable $court\_docket$ is a unique combination of court and charge year. Because in Bexar County each court has one judge, this variable aims to control for the judge the defendant faced.   
The matrix $A_{ikt}$ can also be subdivided into $a_{ik}$, which includes directly observable and measurable attributes of attorneys and the match with their defendants, and $\gamma_{kt}$. Again, $a_{ik}$ contains variables, which are already familiar to us, namly $atty\_exp$, $pct\_appt$, $distance\_to_atty$, $racematch$ and $caseload\_retained$. What we haven't looked at so far is $\gamma_{kt}$, which contains **attorney-by-year fixed effects**. But before we are going to discuss this kind of fixed effects, let's first turn our attention to the subscripts in equation (18) and compare them to subscripts we introduced in the theoretical error component model in exercise 2.2. 

We have three subscripts $i$, $k$ and $t$, compared to the two in the previous theoretical model. The reason is, that we observe multiple cases within a year. The subscript $k$ represents an attorney, while $i$ constitutes a case and $t$ describes the year. To depict the year, we use the time variable $complaintyear$ and to identify each distinct attorney we use $attybarcard$. As we already examined before, there are 726 attorneys over the time period 2005-2014.  

As to $i$, this subscript is kind of deceptive, if you compare it with the error component and fixed effects model literature. As we pointed out in the theory, provided in exercise 2.2, the property of subscripts is, that they refer to repetitive identifiers like individuals and time. But the authors point our that, each case is associated with one defendant. This means there are 64209 unique cases, which contradicts the convention. But let's discuss, why we still need the subscript. As an alternative, we could use the variable $sid$ in our data, to identify each defendant distinctly. 

**Task:**
Use the function `distinct()` from `dplyr` to compute all distinct values of $sid$. 
Assign the call to the variable number_defendants and use `nrow()` to compute the total number of unique accused, number_unique.
Finally compute the difference between the total number of cases and number_unique, dif, and print it out. 
Note that, `distinct()` uses the data as first argument and the column (variable) as second argument.
```{r "2_3__2"}
# First let's use distinct and assign the value to number_defendants
number_defendants <- distinct(___)
# Calculate number_unique
number_unique <- nrow(___)
# Print out number_unique
print(number_unique)
# Compute dif
dif <- nrow(Lawyers)-___
# Print out dif
print(dif)

```

As you see, in 22277 cases the defendant was a repeat offender. Therefore we could use $sid$ as the corresponding variable for $i$ in order to comply with the convention to use repetitive subscripts. In this case, years $t$ would be nested within attorneys $k$ and attorneys $k$ would be nested within $i$ at the top-level. The problem is, we would lose the 41932 observations, because the defendants only appearing once in our data wouldn't contribute any information. Remember from the previous exercise, that there must be some variation in order for identification. So obviously, utilizing our data in such structure would be profiligate.  
Therefore, you should think about $i$ as a case indicator and not as defendant indicator. If you still feel uncomfortable about putting a subscript in a fixed effects model, without the repetitive characteristic, maybe this view helps mitigate your pondering: I'm sure you are familiar with this two notations for an OLS-model.
$$\begin{alignat}{3}
  y_i&=\eta_0 + \eta_1x_i + \epsilon_i \qquad i=1,...,N \qquad \text{(21)} \\\
  \Leftrightarrow y &= \eta_0 + \eta_1x + \epsilon. \qquad \qquad \qquad \qquad \, \quad \text{(22)}
\end{alignat}$$
They are both the same, as the equivalent sign indicates, the only difference is, that in the equation (20) $y_i$ is stated as vector and hence the subscript is dropped. That's the way you can think of $i$ - it indicates the units, which are, in our circumstance, the cases as in equation (19). Note that we can't just drop $i$ (at least not without adjusting our thought process). If we would do so, e.g. $D_i$ would become $D$ and therefore would be invariant in general. That's of course false, because for instance gender is different from case to case.   
By now it should already be clear, that the $i$ subscript in our model has nothing to do with the $i$ subscript from the theoretical error component model in the last exercise. In fact, since $i$ in the last exercise corresponds to the individuals we want to study, it matches the attorneys $k$ in our model.  
You might wonder, why do we rack our brains about a tiny subscript. Well, imagine we would be careless and introduce a fixed effect for $sid$. The `felm()` function wouldn't bother about our mistake and estimate our model, but the results would be highly misleading, as we already explained. 

After we have extensively discussed the structure in our data, we can summarize, that we observe attorney $k$ in $T_i$ time periods, whereby we observe a number of cases in a time period $t$ for each attorney $k$. If you click at the `data` above a code chunck you will recognize this structure. The first column lists attorneys $k$ based on $attybarcard$, the second column lists $t$ based on $complaintyear$ and columns three and four are listing our main outcome variable $guilty$ and the treatment variable $appointed$.  

As a last step, before running the model, let's discuss attorney-by-year fixed effects $\gamma_{kt}$. We already provided an explenation of individual specific fixed effects in the previous exercise, which directly applies to attorneys fixed effects. So the remainder to do, is to work out the difference between attorney fixed effects and attorney-by-year fixed effects. While we have only one attorney fixed effect per attorney, there are $T_i$ attorney-by-year fixed effects for each attorney. That is, while the attorney fixed effect accounts for unobserved heterogenity among attorneys, the attorney-by-year fixed effect accounts for unobserved heterogenity between attorneys and within each attorney across years. We already discussed, which unobserved variables could lead to unobserved heterogenity in the context of attorney fixed effects. Let's shortly do the same within the scope of attorney-by-year fixed effects. Note that such variables must satisfy the condition of no case variation but vary over time or among attorneys.

---

#! addon__quiz__attorney by year fixed effects - job satisfaction

---

#! addon__quiz__attorney by year fixed effects - communication

---

In this specification, the assigned counsel penalty is identified off variation within attorneys who work as both assigned and retained counsel in the same year. If you wonder, why attorneys who just work as retained or even as assigned don't contribute additional information for identification, you should read again Exercise 2.2. But to put it shortly, they get averaged out. To get a better grasp about the various dummys of attonrey-by-year fixed effects, let's compute how many are present and the number of cases in each group. 

**Task:**
Let's use `dplyr` again. First `group_by()` $attybarcard$ and $complaintyear$. 
Afterwards, use `summarize()` and assign the function `n()` to a new variable *ay_cases*.
Additionally compute the mean and standard deviation for every attorney-year combination.
Finally, save the results into *ay_grouped* and show the results.

```{r "2_3__3"}
# I already provided some code
# You have to fill in the middle part, where ___ is located
ay_grouped <- Lawyers %>% 
  ___ %>% 
  summarize(ay_cases=n(),
# now compute assignment rate
  ay_mean=mean(appointed),
# calculating se for further insight
  ay_se=var(appointed)^0.5)
# Show output
nrow(ay_grouped)
# You can select the rows you want to print, if you change the vector in `[`
kable(ay_grouped) %>% kable_styling("striped", "hover", "condensed", position= "left", full_width = F) %>% scroll_box(height = "300px")
summary(ay_grouped$ay_cases)
```

As you have ascertained, there are a total of 4421 attorney-year combinations with 15 cases per year on average. 
Now let's do the same thing, but filter out all attorneys, who worked just as assigned or retained in a year.

**Task:**
We add the `filter()` function and apply it to *ay_grouped* in order to remove all unsuitable groups. 
Note that we defined *ay_group* in the previous task.
The argument of `filter()` is a logical expression based on the assignment rate in each group and filters all
attorney-year combinations without variation. 
The solution is already provided, just press `check`.

```{r "2_3__4"}
# We start, like in the previous task
ay_grouped_filtered <- ay_grouped %>% 
  # Now we filter out all unsuitable groups
  filter(ay_mean < 1 & ay_mean > 0)
# Show outputs again
nrow(ay_grouped_filtered)
# You can select the rows you want to print, if you change the vector in `[`
kable(ay_grouped_filtered) %>% kable_styling("striped", "hover", "condensed", position= "left", full_width = F) %>% scroll_box(height = "300px")
summary(ay_grouped_filtered[,3:5])
```
Firstly, note that we are left only with 2186 groups of attorney-year combinations. Consequently, we lose 2235 combinations of attorney-year. Secondly, in the leftover attorney-years the number of handled cases is on average 20, which is far higher, than the 15 cases from the full data. There is a multitude of possible reasons for this difference. But one possible explenation might be, that we filtered out mostly attorneys, who only work on the privat market, where they don't have to take on so many cases, because they are paid much better. 

**Task:** 
Find out, how many of the filtered out attorney-year combinations belong to retained cases.
Again, use `summary()` to compute the mean cases across the filtered attorney-year combinations.

```{r "2_3__5"}
# Changing the argument of filter appropriately
ay_grouped %>% filter(ay_mean == 1) %>% nrow(.)
# Print summary
summary(ay_grouped %>% filter(ay_mean == 1))
```

Indeed, roughly 60% of the attorney-year combinations, which we filtered out are working on the privat market exclusively and they take on approximately 9.5 cases on average, which is less compared to the full set of attorney-year combinations. Note, that this doesn't validate nor refute our stated thesis, but it suggests where we should pay more attention, when interpreting our model results. By the way, the average number of cases handled for the filtered out attorney-year combinations of only assigned cases amounts to roughly 7.5. It follows, that attorneys who work as both, assigned and retained, take on far more cases, than the other two groups. 

Now let's finally run the model state in equation (20) in order to calculate the assigned counsel penalty $\beta_1$.

**Task:** 
Run the model using `felm()` and save it in a variable called `fe.model`. 
Afterwards eject the output with `summary()`. 
```{r "2_3__6"}
# Run the model and print the results
fe.model <- felm(guilty ~ appointed + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + 
              white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + 
              caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, 
              cmethod="reghdfe", data=Lawyers)
summary(fe.model)
```

The results show the assigned counsel penalty being 8.5%, which isn't that much of a difference compared to the 8% in our pooled OLS with controls from exercise 2.1. The standard errors don't differ much aswell, at least for the treatment variable. If we look at the controls, we recognize that $hisp$ becomes totally unsignificant in comparison to a significance level of 0.1% in the pooled OLS. Remember, that $black$ is the base group of $race$, id est the results from our fixed effects model suggest that being a $white$ defendant on average reduces the probability of being convicted by 4.85% compared to $black$ defendants. On the other hand, the log of $distance\_to\_atty$ gains significance, however, the sign exhibit the direction we would expect. The coefficient indicates, that a 10% increase in the $distance\_to\_atty$ reduces the chances of being found guilty by 0.2%. As the authors state, we would rather expect, that the probability of a favourable outcome would decrease with rising distance to attorney, because of the limited possibilities to meet the counsel. An explenation for this unexpected result might be, that assigned counsels don't exert so much effort, which stands out in less meetings with their clients, so distance doesn't make a big difference in such cases. While attorneys, who get retained, exert more effort, which directly precipitates in more meetings. Also, clients on the privat market have the free choice to hire the best possible counsel and therefore might be more willing to travel longer distances.  

Now let's shortly discuss two attributes of our regression, that shouldn't be evident yet.  
At first, the reasonable question should spark in your mind, why do we include $offense\_num$ and $court\_docket$ as fixed effects alongside $atty\_year$. Revise from exercise 2.2, that the fixed effects model is indeed just an estimator for the error component model. That is, in our model the error term is decomposed into the fixed effects $\gamma_{kt}$ and the disturbance $\epsilon_{ikt}$. Note, how there is no subscript $o$ or $c$ for offense codes or court dockets. That is, we can't decompose our error term into this effects, at least not in the model we came up with in equation (20). Note that, this is just a choice we made based on our interest to study the effects of attorneys. Of course, if we would be particulary interested in courts, judges and offenses, we could come up with another model, where we nest offense codes within court dockets and again use fixed effects. (Of course, our data is not appropirate for such an analysis, cause we lack further explenatory variables related to courts and judges, but the aim of the example is to stress out, that it's a choice not a bulletproof truth.) So, if $offense\_num$ and $court\_docket$ are not fixed effects in the spirit of exercise 2.2, why do we include them in fixed effects sections in the `formula` argument of `felm()`? Remember, that we could estimate an error component model, if we include the fixed effects as dummy variables. As we pointed our, that would be cumbersome, but possible. The variables $offense\_num$ and $court\_docket$ are categorical variables, which if we want to include them, also yield quite some dummy variables. We therefore can use the advantages of `felm()` to estimate this categorical variables just like the attorney-by-year fixed effects in order to avoid computational costs. You should view $offense\_num$ and $court\_docket$ as covariates, they are not fixed effects in our ordinary sense and don't change the identification assumption the of assigned counsel penalty. Remember, we identify $\beta_1$ through variation in cases taken on by the same attorney in the same year. 

info("Dummy, Indicator, Categorical, Factor Variables - what is the right term?") # Run this line (Strg-Enter) to show info

At second, we need to elucidate the standard errors, which are clustered by $attybarcard$. You probably know, how to derive the formula for standard errors in OLS. If not, have a look at the respective info box. The paramount assumption underlying the sampling variance in fixed effects models is $Var(\epsilon_{ikt}|Z_{ikt}, \gamma_{kt})=\sigma^2$. Note, that we denote $Z_{ikt}$ as the vector containing the full set of regressors from equation (20).
Sadly, we can't assume this to hold true in panel data models, generally. Firstly, there could be heteroscedasticity. In that case we necessitate *White* (1980) standard errors. Secondly, there could be autocorrelated error terms among cases or serial correlation across time leading to wrong standard errors we must account for, as *Newey, Kenneth, West* (1987) point out. However, with panel data, we often need to allow the disturbance term to be correlated within clusters, but to fulfill the iid assumption between clusters. The typical example of clusters, are schools, where students are nester within classrooms. Because all students have for example the same teacher, we can't expect test results being independant.   
In our case we will define clusters on the attorney level. That is, we allow for correlation in the disturbance terms among cases and across years within each attorney, but expect no correlation between attorneys. In this case we can state the variance-covariance matrix of the coefficients as
$$\begin{align}
  Var(\hat\beta)=(Z^TZ)^{-1}\left(\sum_{k=1}^KZ_k^T\Sigma_kZ_k\right)(Z^TZ)^{-1}, \qquad \text{(23)}
\end{align}$$

where $Z$ is the matrix over all regressors and observations and $Z_k$ is the matrix over all regressors and observations within an attorney $k$. The variance-covariance matrix of within attorney error terms is denoted as $\Sigma_k=Var(\epsilon_k|Z_k)=\mathbb{E}[\epsilon_k\epsilon_k^´|Z_k]$.   
Note that under homoskedasticity this simplifies to our well-known OLS coefficient variance. Also, you should be aware of the fact, that clustering only is applicable, if we have a high number of clusters. We already examined, that our dataset contains 726 attorneys, so we can put our mind at rest, at least in this respect. One final remark, while the effect of heteroscedasticity on our estimates often is bearable, not accounting for clustered standard errors might greatly harm significance of our results, as *Angrist, Pischke* (2008) showcase in chapter 8 of their book. More precise, *Moulton* (1986, 1990) points out, that in this case our standard errors are seriously downward biased.  
There is an extensive literature out there, on how to estimate $\Sigma_k$. I won't illustrate all the details here, but you should know, that the `felm()` function works based on *Cameron, Gelbach, Miller* (2008, 2012). Moreover, *Correia* (2015) discusses the problems, that arise, when fixed effects are nested within clusters, as in our case where attorney-by-year fixed effects and attorney clusters interfer. Futhermore, you can explore *White* (1984), *Liang, Zeger* (1986) and  *Arellano* (1987) in order to study the fundamentals of cluster robust inference or take a look at *Cameron, Miller* (2015) for a more hands-on approach. By the way, the two arguments `cmethode` and `exactDOF` help us to align the estimator produced by `felm()` and it's companion function `reghdfe` from Stata, which is the statistical language the authors used to produce their results. 

info("Standard Error in OLS") # Run this line (Strg-Enter) to show info

Finally, let's contemplate the other outcome variables, we have put aside until now. Everything we discussed so far applies just as well for this other variables. We will take a look at $dismissed$, $defadj$, $incarcerated$, $reduced$, $lnsent$ and $lnfine$. While all outcome variables are pretty self-explenatory, note that deferred adjudication means that if defendants remain crime-free for a fixed period of time and comply with any other court orders, their case will be dismissed. Of course only defendants are eligible for such a treatment, if they had little or no previous contract with the justice system in the past and who are accused of low-level offenses. 

**Task:**
The code is already provided, just press `Check`. Note, that computations can take a moment. 
Afterwards, go on to the next task in order to produce the output. 
Note that `felm.form()` is my own user defined function. It simply puts together the various parts of the `felm()` syntax. 

```{r "2_3__7"}
# List all outcome variables
depvar <- c("dismissed", "defadj", "incarcerated", "reduced", "lnsent", "lnfine")
regress <- c("appointed male age_offense complaint_hist convict_hist hadbondsman hisp white povrate instability lndist atty_exp pct_appt racematch caseload_retained")
fixed.eff <- c("atty_year offense_num court_docket")

# Use lapply() to loop through depvar in order to run one regression for each outcome
# the results get stored in a nested list structure in model_all
model_all <- lapply(depvar, function(x) {
  felm(felm.form(dep.var = x, 
                 ind.vars = regress, 
                 feffects = fixed.eff,
                 iv = F,
                 endogen = NULL,
                 instruments = NULL,
                 cluster = "attybarcard"), 
       data = Lawyers, cmethod = "reghdfe", exactDOF = T)
}
)

# Assign appropriate names to list elements
names(model_all) <- depvar

# Prepare Output
regressor.labels <- c("Assigned Counsel Penalty", "Male Defendant", "Age at Offense", 
                      "Previous Complaints", "Previous Convictions", "Detained Preadjudication", "Hispanic", "White",
                      "Home Poverty Rate", "Local Borrowing Costs", "Distance from Home to Law Office", "Previous Cases", 
                      "Percent of Cases Assigned", "Ethnic Match", "Average Retained Caseload")

output.label <- c("Dismissed", "Def. Adj.", "Incarcerated", "Reduced Charge", "Log Sent", "Log Fine")

```


**Task:**
Use the command `stargazer()` from the identically named package to show the regression output for *model_all*. Some arguments are already provided, you just need to fill in the model name *model_all*. 
Note that, the computation can again take a moment. 

```{r "2_3__8",results='asis'}
# Produce Regression Table
stargazer(___, type ="html", title="Conditional Regressions on Case Outcomes", out.header=T, covariate.labels= 
regressor.labels, column.labels=output.label, dep.var.labels="", model.names=T, digits=5)
```


---


#! addon__quiz__regression output

---

#! addon__quiz__regression output2

<br>

info("log-level specification") # Run this line (Strg-Enter) to show info

---

#! addon__quiz__regression output t value

Lastly for this exercise, note that for a variable like $incarcerated$ it would be insightful if we could come to know the assigned counsel penalty under the conditional assumption of being convicted before. The reason to do so is obvious, without conviction there can't be an incarceration (although we have 26 such occurrence in our data for some reason). The same logic applies to $lnsent$ and $lnfine$ (where there are indeed no sentences or fines without conviction). Fortunaly accomplishing this task is very straight forward. 

**Task:**
Use `felm()` again to run a set of models on a number of outcome variables defined in *depvar_cond*. 
Use the `subset` argument of the `felm()` function in order to condition only on cases, where the defendant has be found guilty.
Finally, produce the outputs again with `stargazer()`. 

```{r "2_3__9",results='asis'}

# Define new set of dependant variables
depvar_cond <- c("incarcerated", "lnsent", "lnfine")

# This two are the same as in the last task
regress <- c("appointed male age_offense complaint_hist convict_hist hadbondsman hisp white povrate instability lndist atty_exp pct_appt racematch caseload_retained")
fixed.eff <- c("atty_year offense_num court_docket")

# Use lapply() to loop through depvar in order to run one regression for each outcome
# remember to specify the subset argument in order to condition of conviction cases only
# the results get stored in a nested list structure in model_all
model_cond <- lapply(depvar_cond, function(x) {
  felm(felm.form(dep.var = x, 
                 ind.vars = regress, 
                 feffects = fixed.eff,
                 iv = F,
                 endogen = NULL,
                 instruments = NULL,
                 cluster = "attybarcard"), 
       data = Lawyers, cmethod = "reghdfe", exactDOF = T, subset=(guilty==1))
}
)

# Assign appropriate names to list elements
names(model_cond) <- depvar

# Prepare Output
regressor.labels <- c("Assigned Counsel Penalty", "Male Defendant", "Age at Offense", 
                      "Previous Complaints", "Previous Convictions", "Detained Preadjudication", "Hispanic", "White",
                      "Home Poverty Rate", "Local Borrowing Costs", "Distance from Home to Law Office", "Previous Cases", 
                      "Percent of Cases Assigned", "Ethnic Match", "Average Retained Caseload")

output.label <- c("Incar. | Convic.", "Log Sent | Convic.", "Log Fine | Convic.")

# Produce Output
stargazer(model_cond, type ="html", title="Assigned Counsel Penalties for different Case Outcomes conditional on Conviction", out.header = T, covariate.labels = regressor.labels,  column.labels = output.label, dep.var.labels = "", model.names = T, digits=5)

```


## Exercise 3 -- Decomposition of Assigned Counsel Penalty

The main contribution of the authors to the field of criminal justice research is the assessment, how the assigned counsel penalty can be attributed to case characteristics, adverse selection, matching and moral hazard. This is possible through the unique data, that we have at hand, where we can observe the same attorney across cases taken on in the same year. As a result, we can partial out case and attorney characteristics. Hereby, we can draw three conclusions: Firstly, the difference in conditional and unconditional assigned counsel penalty is attributed to case characteristics and adverse selection. Secondly, the residual assigned counsel penalty (remaining difference in outcomes, when attorney is assigned vs. retained) must be due to moral hazard and matching preferences between attorney and client.   

In this section you will learn, how we can decompose the difference between the conditional and unconditional aissgned counsel penalty in exercise 3.1. Afterwards, we will thoroughly examine Matching and Moralthe two explenations for residual assigned counsel penalty in 

## Exercise 3.1 -- Case Characteristics and Adverse Selection


For this exercise it is required to reshape our data. Because I already performed the transformation in order to save us some time, you just need to load the new data set. You can find a short description of the new data below the chunck, aswell as additional information on the transformation process in the corresponding info box. 

**Task:**
Read in the data by clicking `check`.

```{r "3_1"}
Lawyers <- readRDS("Lawyers_dummy.rds")
```

This data reshapes the `factor` variables $atty\_year$, $offense\_num$ or $court\_docket$ into dummy variables, where each factor level corresponds to one dummy. This yields additional 5185 dummy variables. I already performed the transformation. Because neither $atty\_year$ nor $offense\_num$ or $court\_docket$ were consecutive sequences of numbers I changed them to be so. Offense codes display the prefix *oc*, court dockets use *cd* and attorney-year combinations are denoted by *ay*. 

info("Adding Dummy Variables to our Data") # Run this line (Strg-Enter) to show info

In this chapter we will decompose the difference between the unconditional assigned counsel penalty and the conditional assigned counsel penalty. Before we do so on the full set of regressors, as specified in equation (20), let's consider an easier specification in the form of 

$$\begin{align}
  guilty = \beta_0 + \beta_1assigned + \beta_2complaint\_hist+\beta_3hadbondsman+\xi. \qquad \text{(24)}
\end{align}$$
Note, that we dropped the subscripts in this equation. We did this for two reasons: Firstly, this model just serves as an illustration of a phenomenon, that we will discuss hereinafter, and hence we keep it simple, by neglecting the panel data structure and just utilizing pooled OLS over all cases $i$. Secondly, in subsequent tasks we will include all dummys and thus estimate the model with OLS (which is equivalent to the within-estimator as we already learned). The regressor $hadbondsman$ is an indicator variable, which is 1, if a bail was posted and 0 else. 
Let's assume for a moment, that equation (24) represents the true model. In this case, one interesting question, that arises is: What is the influence of the consideration of $complain\_hist$ and $hadbondsman$ on $\beta_1$. A naive approach to assess this question might be to sequential add both variables to the base specification from exercise 2.1. 

**Task:**
In this task we will compute all regressions in order to examine, how sequential addition affects $\hat\beta_1$.
1. Use `lm()` to estimate the base model. 
2. Compute a second regression, where you add $complain\_hist$ to the base specification.
3. Compute a third model, with $hadbondsman$ as addition to base, but without $complain\_hist$.
4. Finally run the full model, as in equation (24).
5. State all results with the help of `stargazer()`.
Note, that step one, four and five are already provided in the code. You just have to correctly specify the arugments `formula` and `data` of the `lm()` command in step two and three. 

```{r "3_1__3",results='asis'}

# Run base specification
naive_base <- lm(guilty~appointed, data=Lawyers)

# add complaint history
add1 <- lm(formula=___, data=___)

# add hadbondsman variable
add2 <- lm(formula=___, data=___)

# Run full specification
naive_full <- lm(guilty~appointed+complaint_hist+hadbondsman, data=Lawyers)

stargazer(naive_base,add1,add2,naive_full, type="html", digits=5)

```


Answer this quiz questions, that will guide you through the computational steps of sequential decomposition. Note, that we are only interested in $\hat\beta_1$ and therefore you only have to look at the first row. Keep attention on the sign. Because $\hat\beta_1$ is decreasing in each step, compared to the base specification you need to include a negative sign in your answers. Because $guitly$ is a dummy variable with values zero and one, let's interpret the numbers as percentage points. Therefore multiply your results by 100 before answering, e.g. you would state the difference between 0.364 and 0.291 as -7.3. 

---

#! addon__quiz__Sequential Decomposition

<br>

---

This table summarizes your calculations. The first column state the effect sizes, when first adding the complaint history and the second column represents the opposite of first adding $hadbondsman$. 

<br>

<table class="table table-striped" style="font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important; text-align:center;">The effect of sequential addition on assigned counsel penalty</caption>
 <thead>
  <tr>
   <th style="text-align:center;">   </th>
   <th style="text-align:center;"> CH into Bail </th>
   <th style="text-align:center;"> Bail into CH </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> Complaint History </td>
   <td style="text-align:center;"> -2.8 </td>
   <td style="text-align:center;"> -0.7 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Bail </td>
   <td style="text-align:center;"> -8.5 </td>
   <td style="text-align:center;"> -10.6 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Total </td>
   <td style="text-align:center;"> -11.3 </td>
   <td style="text-align:center;"> -11.3 </td>
  </tr>
</tbody>
</table>

<br>

---

As you see, if we first control for complaint history the assigned counsel penalty decreases by 2.8 percentage points. However, if we first consider $hadbondsman$, the variation in complaint history _increases_ the outcome gap between assigned and retained attorneys. Note, that there is no mistake, in how I stated the last sentence. Remember, that we are interested in the assigned counsel penalty and not in the other variables. So saying, variation in $hadbondsman$ increases $\hat{\beta_1}$ by 8.5% (in the first column) is equivalent to state, controlling for $hadbondsman$ reduces $\hat{\beta_1}$ by 8.5% (in the first column). 
Maybe you can already imagine what the point is, that I want to make. How does it come, that there are two different effect sizes for the same variable. Is the complaint history reducing the assigned counsel penalty by 2.8 percentage points or just by 0.7%? What we stumble upon here, is sequence-sensitivity of the sequential addition. That means, it makes a difference in which order we add the covariates. But why is that and can we come up with another decomposition to get the insights about what is driving the difference $\hat{\beta_1}^{base}-\hat{\beta_1}^{full}$? 

For the first part of the question - the reason is, that the regressors are not independant, but correlated. To see this let's run some auxillary regressions. 

**Task:**
Run for both added variables $complaint\_hist$ and $hadbondsman$ two regressions. Regress them on appointed in the first one and then add the other as explenatory variable in the second.  
The code is already supplied for the first variable, you have to fill in the analogous code for the second variable.

```{r "3_1__4",results='asis'}

# Auxillary regressions for complaint history
aux1 <- lm(complaint_hist ~ appointed, data=Lawyers)
aux1c <- lm(complaint_hist ~ appointed + hadbondsman, data = Lawyers)

# Now do the same for the bail variable
aux2 <-  lm(formula = ___, data = Lawyers)
aux2c <- lm(formula = , data = Lawyers)

# Finally we retrieve the results
stargazer(aux1, aux1c, aux2, aux2c, type="html", digits=3)
```

Look how the relationship between $appointed$ and $complaint_hist$ changes before and after we include $hadbondsman$. On average the defendants of 1000 assigned counsels got 447 more charges in the past than the defendants of 1000 assigned counsels. If we include the bail variable this difference shrinks to 136. You can draw a similar conclusion for the bail variable. That is, if we add a variable, we can't attribute the whole difference to the treatment variable in question (in our case the assigned counsel penalty), but need to account for the present correlations among sets of explenatory variables. 

After we explained, why we can't use sequential addition to decompose the difference between the base and full model, let's now turn our attention on another decomposition methode, that doesn't suffers from the serious drawbacks, which we just dicussed. The decomposition methode we will us, was proposed by *Gelbach* (2016). He uses the omitted variables formula to assess the gap between $\beta_1^{base}$ and $\beta_1^{full}$, which we state as

$$\begin{alignat}{3}
  \hat{\beta_1}^{base} &= \hat{\beta_1}^{full}+(X_1^TX_1)^{-1}X_1^TX_2\hat{\beta_2} \\\
  \Leftrightarrow \qquad \hat\delta \equiv \hat{\beta_1}^{base}-\hat{\beta_1}^{full} &= (X_1^TX_1)^{-1}X_1^TX_2\hat{\beta_2}, \qquad \qquad \text{(25)}
\end{alignat}$$
for a model $y=X_1\beta_1+X_2\beta_2+\phi$.
Further, if $X_{2k}$ is the k-th column of observations in $X_2$, we can write
$$\begin{align}
  \hat\Gamma_k= (X_1^TX_1)^{-1}X_1^TX_{2k}, \qquad \text{(26)}
\end{align}$$
as the OLS estimated coefficient on $X_1$ from an auxiliary model with $X_{2k}$ as the dependant variable. Because the omitted variables bias formula is linear we can restate equation (24) as
$$\begin{align}
  \hat\delta = \sum_{k=1}^{k_2}\hat\Gamma_k\hat\beta_{2k}=\sum_{k=1}^{k_2}\hat\delta_k, \qquad with \quad \hat\delta_k=\hat\Gamma_k\hat\beta_{2k} \qquad \text{(26)}
\end{align}$$
Note, that $\hat\beta_{2k}$ is the estimated coefficient on $X_{2k}$ in the full model specification and $k_2$ is the number of columns in $X_2$. 

That is, all we have to do in order to decompose the difference in conditional and unconditional assigned counsel penalty is to apply (26). Let's do that first for our small model (equation (24)) with $complaint\_hist$ and $hadbondsman$ as the only two variables in $X_2$. Afterwards, we will follow the same procedure to get a decomposition of our truely full specified model, as it is stated in equation (20). 

Because we already computed everything that is essential to apply the decomposition for the small model, we can straightaway do the computation. 

**Task:**
The code is already provided, just press `check`. 

```{r "3_1__5"}

# Compute effect sizes
effect_ch <- coef(aux1)[2]*coef(naive_full)[3]
effect_bail <- coef(aux2)[2]*coef(naive_full)[4]

# Compute total difference between base and full
delta <- effect_ch + effect_bail

# Compute effect sizes as shares
effect_ch_share <- effect_ch/delta
effect_bail_share <- effect_bail/delta

# Print results
list(delta.sum=delta, delta.mat=c(effect_ch, effect_bail), delta.share=c(effect_ch_share, effect_bail_share))
```

Note, how both effects add up to the total difference $\hat\delta=-11.3\%$ between the base and full regression from equation (24). Also, notice that the effect sizes differ in comparison to the ones obtained by sequential addition. We just acquired all knowledge in order to decompose the main model. But before we do so, let's first rerun the models from equation (1) and (20) in order to restate the difference in unconditional and conditional assigned counsel penalty $\hat{\beta_1}^{base}-\hat{\beta_1}^{full}$. 

**Task:**
The code is already provided, just press `check`.

```{r "3_1__6",results='asis'}
# First run the base regression
fe.base <- felm(guilty ~ appointed, data=Lawyers)

# Now run the full regression
fe.full <- felm(guilty ~ appointed + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + 
              white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + 
              caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, 
              cmethod="reghdfe", data=Lawyers)

# Finally, print the output
stargazer(fe.base, fe.full, type ="html", title="Unconditional and Conditional Assigned Counsel Penalties on Conviction", out.header = T, dep.var.labels = "", model.names = T, digits=3)

```

<br>

---

#! addon__quiz__total assigned counsel penalty

<br>

---


However, there is a little downside on the computational side. In order to do the calculation we need to run many, many regressions. It would be very cumbersome and inefficient to do that by hand. So let's make a virtue out of necessity and learn how to write a function in order to automate the computation. 

We will write the function step-by-step. At first, we should always think about the input required and computed output of a function. The output should look like the one, we computed for the small model (equation (24)). For the input we need arguments for $X_1$ and $X_2$ from the model underlying model in equation (25). Let's denote them as *x1.vars* and *x2.vars*. We also require arguments specifying the outcome variable and the data. We choose *y.var* and obviously *data*. Finally, if we compute so many effects, we are rarly interested in each and everyone. Hence, we necessitate an argument, which accounts for grouping explenatory variables in $X_2$. Let's call it *x2.grouped*.    
We have talked so far about the arguments. A user-defined function however consists of a name, the arguments and finally the body. Let's therefore call our function `gelbach.decomposition()`. Before we go on, let's have a look at what we have so far: 

info("preknit_uZppBqVSMPuz") # Run this line (Strg-Enter) to show info


Prior to writing the body, let's already define the arguments, which we want to commit to the function.
We begin by combining the generated dummy variable names based on $offense\_num$, $court\_docket$ and $atty_year$. 

**Task:**
We use the `matches()` function from `tidyselect` in order to select the relevant names.
The code is already provided, just press `check`.

```{r "3_1__8"}
ay_names <- names(Lawyers)[matches("^ay", ignore.case = F, vars = names(Lawyers))]
oc_names <- names(Lawyers)[matches("^oc", ignore.case = F, vars = names(Lawyers))]
cd_names <- names(Lawyers)[matches("^cd", ignore.case = F, vars = names(Lawyers))]
```

Continue with defining an variable for the *x2.grouped* argument. We want to have two groups. We summarize all names of case characteristics, as denoted in equation (20) by $X_{it}$, in one group and all attorney characteristics, as denoted by $A_{ikt}$, in the other group. 

**Task:**
Define a variable of `class()` `list`, where we rearrange the explenatory variables into groups.
Again, you just need to click on `check`.

```{r "3_1__9"}
group_vars=list(client=c("male", "age_offense", "povrate", "instability", "complaint_hist", "convict_hist", "hadbondsman",
                        "hisp", "white", cd_names, oc_names), 
                atty=c("lndist", "atty_exp", "pct_appt", "racematch", "caseload_retained", ay_names))
```

Now we can define the input arguments.


**Task:**
Specify the arguments, which we deliver to our function `gelbach.decomposition()`. 

```{r "3_1__10"}

data=Lawyers
y.var="guilty"
x1.vars="appointed"
x2.vars=c("male", "age_offense", "povrate", "instability", "complaint_hist", "convict_hist", "hadbondsman", 
"hisp", "white", cd_names, oc_names, "lndist", "atty_exp", "pct_appt", "racematch", "caseload_retained", ay_names)
x2.grouped=group_vars
```

With the exact knowledge on how our input variables look like, let's now write the body of the funcion. 
We will do so in multiple steps and afterwards put the function together.

**Task - Step 1:**
Start simple by extracting the outcome variable *y.var* from the *data*.
Also state the number of elements in the vectors of names *x1.vars* and *x2.vars*.

info("preknit_hnpIDOwPknZq") # Run this line (Strg-Enter) to show info

**Task - Step 2:**
Add the explenatory variables of interest in *x1.vars* and the controls in *x2.vars* into a single design matrix in order to run the full regression afterwards. Save the coefficients from the full regression in a variable *coef.full*. Finally, extract all coefficients of the variables in *x2.vars*. 

info("preknit_YFgGvZUtpaEO") # Run this line (Strg-Enter) to show info

**Task - Step 3:**
For the purpose of running the auxillary regressions - create the design matrix which is based on *x1.vars* purely. 
Also initialize a matrix *delta.mat* to store the effects $\hat\delta_k$ (see equation (26)). 
Now run the auxillary regressions and compute $\hat\delta_k$. Save the results in *delta.mat*.
Finally compute the total gap between the base and full model $\hat\delta$ by summing over all $\hat\delta_k$ and calculate the percentage effect. 

info("preknit_zSrPSnxWKZqf") # Run this line (Strg-Enter) to show info

**Task - Step 4:**
In this step we will take care of the grouping. We start by checking, if the grouping variable *x2.grouped* is specified. If not, output the results *delta.sum*, *delta.mat* and *delta.share* from the last step in a list element. If *x2.grouped* is specified, calculate the group effects *delta.grouped* for each variable in x1.vars by summing over the respective $\hat\delta_k$ in *delta.mat*. Output a list as in step 3, but with *delta.grouped* instead of *delta.mat*. 

info("preknit_owZZIRfSGuWh") # Run this line (Strg-Enter) to show info

**Task - Step 5:**
Now add all together in our function `gelbach.decomposition()`.

```{r "3_1__15"}
gelbach.decompositon = function(data, y.var, x1.vars, x2.vars, x2.grouped=NULL) {

# Step 1
y = data[[y.var]]
n1 = length(x1.vars)
n2 = length(x2.vars)

# Step 2
X.full = cbind(1,as.matrix(data[,c(x1.vars,x2.vars)]))
coef.full = coef(lm.fit(y=y,x=X.full))
beta2 = coef.full[(n1+2):(length(coef.full))]

# Step 3
X1 = cbind(1,as.matrix(data[,c(x1.vars)]))
delta.mat = matrix(0, n1,n2)
rownames(delta.mat) = x1.vars
colnames(delta.mat) = x2.vars
x2.ind = 1
for (x2.ind in seq_along(x2.vars)) {
  gamma = coef(lm.fit(y=data[[x2.vars[x2.ind]]],x=X1))[-1]
  delta.mat[,x2.ind] = gamma * beta2[x2.ind]
}
delta.sum = rowSums(delta.mat)
delta.share = delta.mat / delta.sum

# Step 4
if(!is.null(x2.grouped) & is.list(x2.grouped)) {
  
  group_num <-  length(x2.grouped)
  group_names <- names(x2.grouped)
  delta.grouped = NULL
  
  
  for (i in 1:n1) {
    delta.row <- NULL
  
  for (g in 1:group_num) {
    group <- paste0("group", g)
    assign(group, sum(delta.mat[i,x2.grouped[[g]]]))
    delta.row <- cbind(delta.row, eval(parse(text=group)))
  }
  
  delta.grouped <- rbind(delta.grouped, delta.row)  
  colnames(delta.grouped) <-  group_names
  
  }
  
  rownames(delta.grouped) <-  x1.vars
  delta.share = delta.grouped/delta.sum
  list(delta.sum=delta.sum, delta.grouped=delta.grouped, delta.share=delta.share)

} else {

  list(delta.sum=delta.sum, delta.mat=delta.mat, delta.share=delta.share)

  }
}
```


After we defined the function let's first reassure ourselfs, that we did everything correct. So before running the model with full specifications, let's test the function on the simple model in equation (24). 

**Task:**
Use `gelbach.decomposition()` to decompose the model in equation (24).

```{r "3_1__16"}

gelbach.decompositon(data=Lawyers,
                     y.var = "guilty",
                     x1.vars= "appointed",
                     x2.vars = c("complaint_hist", "hadbondsman"),
                     x2.grouped = NULL)

```

It looks like everything is working fine, since we get the same results, as the one we calculated manually. 
So let's apply our decomposition function on the full model in equation (20).

**Task:**
Use the newly defined function `gelbach.decomposition()` to decompose the model stated in (20) in relation to the base model with $appointed$ being the only explenatory variable. 
Remember, that we already assigned all variables to the argument names. 
Note, that this computation really needs a lot of time. 

```{r "3_1__17"}
gelbach.decompositon(data=data,
                     y.var = y.var,
                     x1.vars= x1.vars,
                     x2.vars = x2.vars,
                     x2.grouped = x2.grouped)
```


As you see, we get the same results as the ones in figure 2 in the authors paper. However, our function isn't that evolved compared to the one, which they used. For example, we didn't calculate clustered standard errors and hence can't report confidence intervals like they do, so it would be definitly well worth taking a look at their table. Because the computation is very time consuming we won't repeat it for the other outcome variables. Doing so wouldn't be by any means difficult, all we would have to do is changing the *y.var* argument to the name of the outcome variable we want to consider next. Note, that you can take a look at the decomposition of the four main variables analyzed by the authors in the table below. Also, the results are again reported in percentage points with percentage changes in relation to the base specification in parenthesis. 

info("Difference between Percentage Points and Percent") # Run this line (Strg-Enter) to show info

<br>

<table class="table table-striped" style="font-size: condensedpx; width: auto !important; margin-left: auto; margin-right: auto;">
<caption style="font-size: initial !important; text-align:center;">Decomposition Results</caption>
 <thead>
  <tr>
   <th style="text-align:center;">   </th>
   <th style="text-align:center;"> Convicted </th>
   <th style="text-align:center;"> Dismissed </th>
   <th style="text-align:center;"> Def. Adj. </th>
   <th style="text-align:center;"> Inc. | Convic. </th>
  </tr>
 </thead>
<tbody>
  <tr>
   <td style="text-align:center;"> Base Specification </td>
   <td style="text-align:center;">  18.3 </td>
   <td style="text-align:center;"> -13.5 </td>
   <td style="text-align:center;"> -5.6 </td>
   <td style="text-align:center;">  7.5 </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Case Characteristics </td>
   <td style="text-align:center;"> -6.2 (34%) </td>
   <td style="text-align:center;">  4.2 (31%) </td>
   <td style="text-align:center;">  2.0 (36%) </td>
   <td style="text-align:center;"> -5.8 (77%) </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Attorney Characteristics </td>
   <td style="text-align:center;"> -3.6 (20%) </td>
   <td style="text-align:center;">  3.2 (24%) </td>
   <td style="text-align:center;">  0.4 (7%) </td>
   <td style="text-align:center;">  0.6 (8%) </td>
  </tr>
  <tr>
   <td style="text-align:center;"> Full Specification </td>
   <td style="text-align:center;">  8.5 (46%) </td>
   <td style="text-align:center;"> -6.1 (45%) </td>
   <td style="text-align:center;"> -3.2 (57%) </td>
   <td style="text-align:center;">  2.3 (31%) </td>
  </tr>
</tbody>
</table>

<br>

The table shows, that we can explain between 43 and 55 percent in the assigned counsel penalty differences through case/defendant characteristics and attorney characteristics for the variables $guilty$, $dismissed$ and $defadj$. Regarding the incarceration conditional on all cases with a guilty judgement, case/defendant characteristics and attorney characteristics explain together 69% in the gap between the base and full model specification.  
If we take a look at the convictions in the first row, the second and third columns reveal that case characteristics explain 34 % of the difference between the base and full specification while adverse selection account for 20% of the unconditional assigned counsel penalty. So, there remains a residual assigned counsel penalty of 8.5 percentage points, that we can't explain by case characteristics nor attorney characteristics. 
On average, indigent clients are 13.5 percentage points less likely to have their cases dismissed than non-indigent clients. Controlling for client and case characteristics accounts for 31 % of the unconditional assigned counsel penalty. Variation in attorney characteristics is responsible for an 24% increase in the gap between the full and base model. Note, that the authors points out, that even the 95 % confidence interval only yields that 43 % of the unconditional disparity is due to adverse selection.     
The Likelihood to receive deferred adjudication is for clients, that are represented by assigned counsels on average 5.6 percentage points lower. Variation in case characteristics explains 36% of the disparity. Whereas, adverse selection is virtually negligible, accounting only for 7% of the gap. The remaining conditional assigned counsel penalty amounts to 3.2 percentage points or 57%.   
A glance at the last row unveils that the unconditional assigned counsel penalty amounts to 7.5 percentage points in terms of incarceration (conditional on conviction). Most of this disparity can be explained by case characteristics, which account for 77% of the gap in incarceration outcomes. Again adverse selection plays a very minor role in explaining the differences between the base and full specification. Indeed, controlling for attorney characteristics seems to increase the gap by little. Finally, the residual assigned counsel penalty yields 2.3 percentage points.  
We can conclude, that observed case characteristics play an important role in explaining the relatively worse case outcomes among indigent clients with assigned counsel. On the other hand, the influence of adverse selection is greatly dependant on the outcome variable in question. While there is some noticeable impact on the disparity of case dismissal and conviction, such effects can't be established for deferred adjudication and conditional incarceration. Further remarkable is, that adverse selection never shows the same importance as case characteristics do. This is to some extent contrary to the previous work of *Iyengar* (2007) and *Roach* (2014). 

After we discussed how we can assess the gap between the unconditional assigned counsel penalty $\hat{\beta_1}^{base}$ from equation (1) and the conditional assigned counsel penalty $\hat{\beta_1}^{full}$ from equation (20) by means of decomposition. Let's now make a start on the residual assigned counsel penaltys. That is, since we determined the effects of case characteristics and adverse selection we are only left with matching preferences and moral hazard as the remaining causes for the remaining gap. You can now go on to the next exercise, where we start with matching preferences.  


## Exercise 3.2 -- Matching Preferances

As the authors state, part of the residual penalty could arise from the inability of indigent clients to match with attorneys based on their personal preferences. The reason behind this assumption stems from the idea, that mutual trust facilitates communication between a client and his or her attorney, which in then again may help lawyers to uncover relevant facts, witnesses, alibis or extenuating circumstances regarding a case. The authors further reason, that the enhanced communication due to matching preferences could help to ensure that the defendant behaves in a way that reduces the probability that he or she will be convicted or incarcerated, such as showing up on time, showing adequate demeanour in court and refraining from suspicious activity while the case unfolds. The policy question that arises thereupon is, should indigent defendant select their own counsel in opposition to an assignment by third partys and judges. Let's try to assess this question with the help of our data. First let's look at some summary statistics in order to figure out, if there are any differences between private attorneys, which get choosen by non-indigent clients, and CJA panel attorneys, who get assigned by a third party. Note, that this tables provide an more elaborated view on the differences in attorney characteristics, that we already seen in the exercise 1.2. 

**Task:**
Simply click check and look at the tables. You can switch between them either by wiping with your mouse or clicking on the radio buttons beneath.

```{r "3_2",results='asis'}
includeHTML("indexx.html")
```

<br>
<br>
<br>


As you might already figured out, that there are substantial differences in attorney characteristics. Firstly, there seems to be an overall higher preferance among non-indigent clients to retain an attorney of same race. Secondly, both, hispanic and black defendants are more likely to retain an attorney of the same race than to be assigned one. However, this relationship doesn't apply for white defendants. Moreover, there is a subtle preferance, overall and over all races, for male attorneys. On average, the office of an appointed counsel is about seven miles further away from the resident of it's client, when compared with retained attorneys. The experience of retained attorneys exceeds the experience of assigned attorneys, both, in years and cases. Finally, clients seem to prefer to retain counsels, who are rather specialized in the category of crime the defendant is accused off.   

In the course of this exercise we will scrutinize whether revealed preferences in terms of attorney characteristics are decisive for the assigned counsel penalty. To that, we will use the conditional assigned counsel penalty from equation (20) on the meanwhile well known set of explenatory variables. In order to carry out the investigation we will add additional interaction terms to $appointed$.   

We start by examining the race dimension. We already discussed that race preferences differ along black, hispanic and white defendants. To account for that we will run regressions on each subgroup separately. Moreover, we will run in each subgroup two different specifications. The first specification aims at showing the average differences for a racial match relative to an attorney of a different race. The second model answers whether there is an average difference for having an attorney of each of the two other ethnicities with respect to a racial match. Because there are a total of 24 regressions (2 specifications x 3 subgroups x 4 outcomes) we will again focus on conviction as the primary outcome variable and state the results for the other outcomes afterwards. So let't get into it. 

First we will consider the influence of black defendants being defended by black attorneys on the residual assigned counsel penalty $\hat\beta_1$ from model (20). Therefore, we will use the variable $apt_bb$, which is a dummy variable with the values 1, if the attorney is appointed and both, defendant and attorney, are black and 0 else. Note that the following few tasks replicate table 3 from the authors paper.   
The next step is to run the regression, where we add the interaction term $appointed*apt\_bb$ to the specification in equation (20). But before doing so, let's first read in the data. 

```{r "3_2__2"}
Lawyers <- readRDS("Lawyers.rds")
```

**Task:**
Run again the model in equation (20), but with the additional interaction term added. 
The interaction term is already included in the regression code below.
Fill in the missing `subset` argument with the help of the $race$ variable, which comprises the values B=Black, W=White, L=Latino/Latina (hispanic).
Recall, that we are currently interested in the race preferences of black defendants. 

```{r "3_2__3"}
# Run the regression
fe.bb <- felm(guilty ~ appointed + apt_bb + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers, subset=(___))
summary(fe.bb)
```

Firstly, note that we are unable to estimate some of the regressor coefficients. The reason for this is perfect mutlicolliearity, thus we will omit $hisp$, $white$ and $racematch$ from subsequent regressions. Either way, in the first place we are interested in the coefficients of $appointed$ and $apt_bb$. While the coefficient of $appointed$ represents the conditional difference in convictions between the average assigned and retained attorneys (assigned counsel penalty), the coefficient of $apt_bb$ is the average conditional difference in getting assigned a black attorney as opposed to getting assigned an hispanic or white attorney. Hence, we denote the sum of both effects as the assigned black counsel penalty, which sums up to 8%. Consequently, the difference between the assigned cousel penalty and the assigned black counsel penalty yields 1.5%. Note however, that this coefficient is not significant. 

#! start_note "Assigned Black Counsel Penalty for other Outcomes"

In order to get the results for other outcomes, we just need to change the dependant variables (and in the case of $incarcerated$ further subset our data based on $guitly$)

```{r "3_2__4",results='asis'}
# Enter your code here.
```


The results for the other outcomes are analogous to the one for $guilty$. Additional on being indigent, getting an black attorney assigned as black defendant always increases the assigned counsel penalty. In the case of conditional incarceration the coefficient on $appointed*apt\_bb$ suggests an additional increase of assigned counsel penaly by the factor 2. But again, all results for the additional effect are not significant. 
#! end_note


Although, the results weren't significant, we can assess the question whether black attorneys obtain less favourable outcomes among other races of defendants too. So let's run two additional regressions. In the first we add $black\_appt$ and $white\_app$ to the regressors and subset on hispanic defendants. In the second regression we add $black\_appt$ and $white\_appt$ and subset on white detendants. 

**Task:**
Run both regressions. Fill in the `subset` argument with appropriately conditioning $race$. Recall, that race takes on the three values "B", "L" and "W".
The code is mostly complete, just fill in the right subsetting condition.

```{r "3_2__5",results='asis'}
# Regression on hispanic defendants subsample
fe.lbw <- felm(guilty ~ appointed + black_appt + white_appt + male + age_offense + complaint_hist + convict_hist + hadbondsman + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers, subset=(___))

# Regression on white defendants subsample
fe.wbl <- felm(guilty ~ appointed + black_appt + hisp_appt + male + age_offense + complaint_hist + convict_hist + hadbondsman + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers, subset=(___))

# Output
regressor.labels <- c("Assigned", "Assigned x Black Atty", "Assigned x Hisp Atty", "Assigned x White Atty")

stargazer(fe.lbw, fe.wbl, type ="html", title="Ethnicity Dependant Regressions - Hispanic and White Defendants", 
            covariate.labels = regressor.labels, digits=4, 
            keep = c("appointed", "black_appt", "hisp_appt", "white_appt"), keep.stat = "n", star.char = c("+", "*", "**", "***"), order = c("appointed", "black_appt", "hisp_appt", "white_appt"), star.cutoffs = c(0.1, 0.05, 0.01, 0.001), notes = c("+p<0.1; *p<0.05; **p<0.01; ***p<0.001"), notes.append = F)

```


The results show that, if hispanic defendants get assigned an black attorney they achieve a better outcome in conviction compared to getting assigned an attorney of the same race. Contrary, if white defendants get legally represented by black assigned attorneys they have to expect a worse outcome compared to getting assigned an white assigned attorney. But yet again the results don't have any kind of significance. Therefore we can conclude, that while black defendants prefer to hire an attorney of the same race compared to getting assigned one, they don't benefit from that. Further we can rule out that black attorneys perform systematically worse than their colleagues. 

We could do the same analysis for hispanic and white defendants aswell, but they yield also unsignificant results, hence let's skip that and put on record that the preferences we have seen in the cube of tables doesn't mirror meaningful reductions of assigned counsel penalty. You can have a look at the info box, where you find a table with all ethnicity-based regressions, if you want to reassure yourself, that this conclusion holds. 

info("Full Set of Ethnicity-based Assigned Counsel Penalty Regressions") # Run this line (Strg-Enter) to show info


Now let's do a similar assessment for the other revealed preferences, starting with the preference for male attorneys. 

**Task:** 
We begin by rescaling some variables in order to derive more sound interpretations. 
Just press `check`.

```{r "3_2__6"}
Lawyers$reg_exp <- Lawyers$experience/10
Lawyers$pct_spec <- Lawyers$percent_cases_offense_specific*100
Lawyers$pct_appt <- Lawyers$pct_appt*100
```


For representation purposes we will just focus on $guilty$ again and begin with a regression where we an interaction term for the variable $attymale$, which indicates the attorneys gender. Note that the authors also include the new scaled $reg\_exp$ as additional explenatory variable, so we will do the same. 

**Task:**
The code is already complete. Run the regression by clicking on `Check`.

```{r "3_2__7"}
# Run the Regression
fe.attymale <- felm(guilty ~ appointed + appointed*attymale + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + reg_exp +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)
```

The results show, that for indigent defendants a female attorney is hardly more favourable than a male counsel. Additionally the result is not significant at all, thus we have no evidence to assume that attorney gender has an impact on the residual assigned counsel gap. 

We will now proceed identically for the variables $lndist$, $reg\_exp$, $atty\_exp$ and $pct\_spec$ by interacting them with $appointed$ and regress on the full data. Note that $reg\_exp$ states the decades which the respective attorney is working in his profession since his or her bar admission. The $pct\_spec$ variable displays for every case the percentage of same offenses the attorney has dealt with in the past. Note that We will do all regressions together in the following task. 

**Task:**
The code is already complete. Run the regressions by clicking on `Check`.

```{r "3_2__8",results='asis'}
# Run all Regressions
fe.lndist <- felm(guilty ~ appointed + appointed*lndist + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + atty_exp + pct_appt + racematch + reg_exp +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)

fe.reg_exp <- felm(guilty ~ appointed + appointed*reg_exp + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + reg_exp +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)

fe.atty_exp <- felm(guilty ~ appointed + appointed*atty_exp + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + reg_exp +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)

fe.pct_spec <- felm(guilty ~ appointed + appointed*pct_spec + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch + reg_exp +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)

stargazer(fe.lndist, fe.reg_exp, fe.pct_spec, type ="html",  
            covariate.labels = c("Assigned x Log Distance", "Assigned x Experience", "Assigned x Specialization"), digits=4, 
            keep = c("appointed", "appointed*lndist", "appointed*reg_exp", "appointed*pct_spec"), keep.stat = "n", star.char = c("+", "*", "**","***"), order = c("appointed", "appointed*lndist", "appointed*reg_exp", "appointed*pct_spec"), star.cutoffs = c(0.1, 0.05, 0.01, 0.001), notes = c("+p<0.1; *p<0.05; **p<0.01; ***p<0.001"), notes.append = F)
```

The regression output shows, that the distance between an assigned attorney and his or her defendants resident isn't significant. Years of experience however is significant at the 10% level. But remember, that we rescaled the variable by a factor of 10. Thus the interpretation of the coefficient is, that on average 10 years of experience lead to an reduction of the residual assigned counsel penalty by 1.2%. Finally, we get highly significant results for the specialization variable, but since the magnitude of the coefficient is negligibly small offense specialization doesn't have much implications on the residual assigned counsel penalty. The same conclusion can be applied to $atty\_exp$, which captures the attorney experience by the number of cases. Altough the coefficient is highly significant, the effect size is rather minor. On average, assigned attorneys who have resolved 100 cases show an approximately 0.05 percentage points lower residual assigned counsel penalty. This is to some extent surprising, if we consider the overall strong preferences for attorneys with many resolved cases. If we change the oucome variable, we wouldn't obtain other results, therefore we abandon it at this point and refer to table 5 in the authors paper. 

At the end of this exercise we can conclude, that although there are some strong preferences on the private market for some attorney characteristics, they don't seem to play an important role in reducing the residual assigned counsel penalty.  
Hence, we are left with only one more possible explenation from the initial four - moral hazard, which we are going to examine in the next exercise. 



<div class="garbage">

info("ignore this, this is just a garbage collector for the tables cube") # Run this line (Strg-Enter) to show info
</div>



## Exercise 3.3 -- Moral Hazard

We begin by reading in the data first.

```{r "3_3"}
Lawyers <- readRDS("Lawyers.rds")
```


Now, let's start this exercise off by recalling that the residual assigned counsel penalty is about 8.5%. This constitutes an upper bound for moral hazard. One possible explenation that underlies the issue of moral hazard could be differences in attorney compensation. According to the authors the median criminal defense attorney in San Antonio (the capital of Bexar County) charged 200$ per hour on the private market in 2015. Fees are primarily set based on the severity and complexity of the case. The authors further state, that flat fees are common in private criminal law and are set primarily on the severity and complexity of the case, e.g. possession of less than one gram of marijuna would amount to 1000-4000\$. Consequently, at an hourly rate of 200\$, a retained attorney would expect to work 5-20 hours.   
Assigned attorneys in our data have been compensated based on a fee schedule. Fees are structured in the three different categories, based on the severity of offense, state jail and 3rd degree felonies, 2nd degress felonys or first degree felonys. Independent of the severity all assigned attorneys get 100$ for an initial jail visit. Further they are compensated on hourly rates for court appearance, evidentiary hearings, trials and time out of court. However, hourly rates only span the range of 50-125\$. Flat fees compose the main composation source. However, between 2005 and 2009 only cases resolved by plea bargains or representing MTR cases (Motion to Revoke Probation) have been eligible for flat fee compensation. Resolving a case via plea bargain yieled 400-750\$ and representing MTR cases yielded 175-300\$, depending on the severity of the offense. After 2009 the flat fees have also become available for assigned attorneys who achieved the case being dismissed and increased for MTR cases. However, after 2015 all types of case resolution became eligible for flat fee compensation, which are 6% of our sample. Appointed attorneys choose the flat fee compensation in 75% of time. The reason is, according to a survey conducted by authors, that flat fee requests are not subject to review, whereas a judge can adjust, or even refuse, the hours requested by a lawyer. Another reason might be the possiblity to raise the hourly wage by resolving cases faster and getting compensated by a flat fee. For example, resolve a second degree offense case by guilty plea renders possible a 500\$ flat fee compensation, incorporating that the median private market hourly wage is 200\$, the assigned counsel would calculate to spend 2.5 hours in order to break even.  
As a consequence of the lower compensation for assigned cases, attorneys may exert less effort and focus more on their private clients. Hence, they haven an incentive to spend less time on assigned cases and may aim to seek a fast resolution. In this light, we can assess moral hazard by using $case\_length$ as a proxy for lawyer effort. Case length is calculated as the difference between complaint date and the earlier of judgement date and adjudication date. Let's run a simple analysis to shortly discuss the effect of case length on being convicted. 

**Task:**
Run a simple (pooled) OLS with $case\_length$ as regressor on $guilty$ as dependent variable. 
Use the `lm()` function and save the result in a variable *lm.case*. 
Finally use `summary()` to produce the regression output.

```{r "3_3__2"}
# Use lm and fill in the correct syntax
lm.case <- ___
# Produce regression output
summary(___)
```

Of course we already now, that our estimate of the $case\_length$ coefficient is biased, but the point at this point is just to get an intuition of the relationship and whether it aligns with our expectations. So, the regression shows, that on average an increase in the case length by 10% yields a reduction in the probability of being found guilty by 1.1 percentage points. Well, as mentioned, we shouldn't entirely trust the magnitude, but the direction of sign seems accurate. Delaying a case can lead to some benefits like unexpected testimonies or a deal with the prosecutor. Therefore, attorneys may exploit various strategies in order to delay a case, e.g. by requesting continuances for consultation purposes or for psychiatric evaluations, filing numerous motions, and using the discovery process to postpone hearings (as mentioned by the authors.) So the natural thing to ask hereinafter is, whether there is a difference in case length between appointed and retained attorneys.   

**Task - Step 1:**
First of all, recall, that $case\_length$ is stated in log days. That is, we can't just calculate the mean $case\_length$ in both groups, appointed and retained counsels, since that wouldn't yield the arithmetic average of case length in days. Instead, first define a new variable *caselength* (and attach it to our data), that displays the case length in days, you can achieve this by applying the `exp()` function on $case\_length$. 

```{r "3_3__3"}
# Specify the right-hand side of the expression
caselength <- ___
```


**Task - Step 2:**
Now, calculate the `mean()` for each group of attorneys.
Note, that we follow the procedure of the authors and remove MTR cases from our sample first.
The code is almost complete, you just need to fill in the computation of the `mean()` $caselength$. 

```{r "3_3__4"}
# First, filter the data to exclude MTR cases
Lawyers <- Lawyers %>% filter(___)

# Now fill in the missing part in the chain
Lawyers %>% group_by(appointed) %>% summarize(mean_caselength=___)
```

We obtain the result, that cases are on average 65% shorter when the counsel is assigned as opposed to retained. That seems quite significant. Of course we need to revise this statement under consideration of case/defendant and attorney characteristics again, as we did with $guilty$ as outcome variable. Hence, let's compute the conditional assigned counsel penalty with respect to $case\_length$. 

**Task:**
Use the `felm()` function with the familiar set of regressors from equation (20) and the dependant variable $case\_length$. 
Then apply `summary()` to the variable, where the results are stored to print the output to console. 

```{r "3_3__5"}
# Now run regression on case_length
fe.case_length <- felm(case_length ~ appointed + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)

# Print regression results
summary(fe.case_length)

```

As you see, on average assigned cases get resolved 13.4% (14.4% if we use the correct log-level interpretation) faster than cases where the attorney was retained. Of course we could use `gelbach.decomposition()` function from exercise 3.1 to decompose this result, but we already experienced, that it takes a lot of time to run, so instead we will refer to the results from the authors. They state, that 51% of assigned counsel penalty for case length can be attributed to case characteristics and 28% are explained by adverse selection.   

Additionally, we can incorporate into our analysis that their might be a difference in case length based on the bail status. 

**Task:**
Calculate the `mean()` twice, ones for the defendants who were detained preadjudication and ones for the released defendants preadjudication.
The code is almost complete, you just need to fill in the computation of the `mean()` for $caselength$. 

```{r "3_3__6"}
# fill in the missing part in the chain
Lawyers %>% group_by(hadbondsman) %>% summarize(mean_caselength=___)
```

The results show, that cases without a bail posted are 55% shorter than cases where the defendant was released preadjudication. 
Based on this results, let's calculate the conditional assigned counsel penalty again, ones for the subsample of no bail defendants and ones for the subsamples with bails posted. Note that in both subsamples also MTR cases stay removed. 

**Task:*
Firstly, filter the data to include only cases where bail was posted. Then run regression using `felm()` again with $case\_length$ as response variable, but with the new subset of data. 
Secondly, filter the data to include only cases where bail was not posted. Now run regression on this respective subsample. 
Finally print the results with stargazer. 
The code is already mostly complete, you just need to accordingly `filter()` the second subsample.
Recall, that the second subsample is the one, with defendants detained. 

```{r "3_3__7",results='asis'}
# Firstly, filter the data to include only cases where bail was posted
Lawyers_bail <- Lawyers %>% filter(hadbondsman==1)

# Now run regression on case_length
fe.case_length_bail <- felm(case_length ~ appointed + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers_bail)

# Secondly, filter the data to include only cases where bail was not posted
Lawyers_nobail <- Lawyers %>% filter(___)

# Now run regression on case_length again
fe.case_length_nobail <- felm(case_length ~ appointed + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers_nobail)

# Print regression results
stargazer(fe.case_length_bail, fe.case_length_nobail, type ="html", title="Assigned Counsel Penalty of Case Length based on Bail Status", digits=4, keep = c("appointed"), keep.stat = "n", star.char = c("+", "*", "**", "***"), order = c("appointed"), star.cutoffs = c(0.1, 0.05, 0.01, 0.001), notes = c("+p<0.1; *p<0.05; **p<0.01; ***p<0.001"), notes.append = F)

```

The results show a blatant difference in the influence of bail status on case length among appointed defendants. On the one hand cases, where the attorney is assigned and the defendant is released preadjudication get resolved on average 13.9% faster than cases, where the attorney is retained and the defendant is released. On the other hand, cases with appointed attorneys and no posted bail endure on average 23.9% longer than cases with retained attorneys, where no bail was posted. The reason for this disparity is the aspiration of detained defendants to get to the case resolution quickly with the potential of a favourable outcome. However, in the situation where the defendant has posted a bail he or she has an interest in delaying case resolution, as we pointed out earlier in this exercise. The fact, that there is a significant assigned counsel penalty on case length for released defendants indicate, that the disparity is driven by attorney interest for quick case resolution, which contradicts the defendants aspiration. This finding aligns with our discussion of the fee structures of assigned attorneys, which induce them to resolve cases faster in order to break even. 

Let's futher investigate lawyers compensation. We can exploit the circumstance that flat fee compensation changed during the sampling periode of our data by assessing the implications for attorney behavior. Therefore take a look at the image below. 

<br>

<center>

![fee_structure](fee_structure.png)

</center>
<br>

In the case of case dismissal as outcome variable we can't observe the residual assigned counsel penalty to change on the basis of the differenct fee structure after july 2009. This is so some extent suprising, because the fee structure change espicially aimed at allowing flat fee compensaion for dismissed cases. But it gets plausible, if we recall, that successfully completed periods of deferred adjudication results in case dismissal. Because the prosecuter will accept a deferred adjudication more likely than a immediate case dismissal the change in fee structures has a much higher impact on deferred adjudication, compared to case dismissals. Moreover, after the flat fee structure for dismissed cases was in place the assigned counsel penalty for guilty pleas notably declined. That indicates, that lawyers behavior changes duo to the compensation structure. 

The findings in this exercise suggest, that moral hazard could be well suited in explaining the gap between the outcome of retained as opposed to assigned attorneys. We found that, assigned lawyers are more likely to resolve cases faster and presumed, that compensation might be an aspect of that. Indeed, after contemplating residual assigned counsel penalties in the time before compensation structure was change in Bexar County and comparing it with the time after, we can find remarkable shifts in assigned counsel gaps for the outcomes sensitive to the change. 



## Exercise 4 -- Investigating Recidivism 

You know it, the first thing to do is, reading in the data.

**Task:**
Read in the data by clicking `check`.

```{r "4"}
Lawyers <- readRDS("Lawyers.rds")
```

In the final exercise we will assess the implications of the assigned counsel penalty on recidivism of defendants. Therefore, we will consider **attorney-specific** assigned counsel penalties and their impacts on the future recidicism of clients. Because we identify attorneys distinctly by $attybarcard$, but the bar numbers aren't so convenient to handle, let's change them to a consecutive numbering. 

**Task:**
Generate a consecutive numbering of Lawyers, save it in a variable *atty* and attach it to the data. Then, change the `class()` of the newly defined variable to `factor`.
One way to do this, is by utilizing the `cumsum()` and `diff()` function. 
If we apply `diff()` on the ordered vector $attybarcard$, we get a value of one always when another attorney appears in the data. The `cumsum()` function returns the accumulated sum vector, which has the same length as our data, if we additionally add a 1 at the start (since the `diff()` is not defined for the first row).
The code is already provided, just press `check`.

```{r "4__2"}
Lawyers$atty <- cumsum(c(1,diff(Lawyers$attybarcard) != 0))
```

The following steps will guide you through the process in order to replicate figure 4 from the authors paper, which shows the linear relationship between the attorney-specific assigned counsel penalty and the probability that an attorneys client recidivates. Note that we will focus on the code first and give an interpretation of the resulting figure afterwards.

**Task - Step 1:**
Firstly, change $atty$ to `class()` `factor`.
Secondly, rerun the model specification stated in (20) with the supplement of adding interaction terms, between $appointed$ and $atty$. 
The code is already provided, just press `check`. Note, that the computation takes some time.

```{r "4__3"}
Lawyers$atty <- factor(Lawyers$atty)
fe.atty <- felm(guilty ~ appointed:atty + male + age_offense + complaint_hist + convict_hist + hadbondsman + hisp + white + povrate + instability + log(distance_to_atty) + atty_exp + pct_appt + racematch +  caseload_retained | atty_year + offense_num + court_docket | 0 | attybarcard, exactDOF = T, cmethod="reghdfe", data=Lawyers)
```

The regression output shows for some attorneys `NA`. As we discussed previously, there has to be variation in assigned and retained cases within attorneys (and years). If there is no variation they don't contribute to the estimation and hence are labelled with `NA`. 


**Task - Step 2:**
Now let's save all coefficients that we obtained from the last regression in our data. We want to match each attorney-specific coefficient to the respective attorney. 
Use the `dplyr` functions `group_by()` and `mutate()` to achieve this. 
While the argument to `mutate()` is already provided, you need to fill in the right argument for `group_by()`. 

```{r "4__4"}
# Group by atty in order to assign each attorney his or her corresponding coefficient
Lawyers <- Lawyers %>% group_by(___) %>% dplyr::mutate(attybeta = fe.atty$coefficients[-c(1:14),][atty])
```

**Task - Step 3:**
Now run a regression on the variable $recidicism3$, which takes the value 1, if the defendant gets charaged with a felony again within three years and 0 else. The explenatory variables are provided in the chunck. Note, that we speficy the `subset` argument of the `lm()` function such that only defendants who hasn't been incarcerated get considered in the regression.

```{r "4__5"}
# Now run the linear model (already completely provided)
recid3 <- lm(recidivism3 ~ male + age_offense + complaint_hist + convict_hist + 
    hadbondsman + race + povrate + instability, data=Lawyers, subset=incarcerated==0)

# Print Output
summary(recid3)
```

**Task - Step 4:**
This step calculates the residuals for all cases in our **complete** sample of all 64209 cases. 
The solution is attached to our data. We also add two columns to our data, where we save the residual, if the observation was used in model estimation and the attorney is assigned or
Click on `check` to run the chunck. 

```{r "4__6"}
Lawyers$recid_resid <- Lawyers$recidivism3-predict(recid3, Lawyers)
Lawyers$resid_appt <- ifelse(Lawyers$appointed==1 & Lawyers$incarcerated==0, Lawyers$recid_resid, NA)
```

**Task - Step 5:**
In this chunck the mean residuals for option a) and option b) get averaged for each attorney. Further, we compute the assignment rate for each attorney and append the solution to our data with `mutate()`. 
Just press `check`. 

```{r "4__7"}
# Compute the mean residuals of option a) and option b) for each attorney
Lawyers <- Lawyers %>% group_by(atty) %>% 
  mutate(mean_resid_appt = mean(resid_appt, na.rm=T),
         
         # Compute the Assignment rate
         shr_assigned = mean(appointed))
```

**Task - Step 5:**
Because we want to use an weighted least squares regression in the next step, we need to determine the weights first. We do this by using *shr_assigned* from the last computation and muliply it with *1-shr_assigned*. By this, we reassure that attorneys who have a balanced share of retained and assigned cases get higher weights. The maximum weight is 0.25, reserved for attorneys with equal retained and assigned cases and the minimum is 0, reserved for counsels with only retained or assigned cases. We also require the total amount of cases handled by each attorney, *atty_n*. The chunck already contains the complete code, so click on `check` and continue. 

```{r "4__8"}
# Define weights
Lawyers$shr_assigned_i <- Lawyers$shr_assigned*(1 - Lawyers$shr_assigned)

# Define total case number for each attorney
Lawyers <- Lawyers %>% group_by(atty) %>% mutate(atty_n = row_number())
```

**Task - Step 6:**
In this step we will run the weighted least sqaures regression. 
We calculated the weights already in the last step. 
The dependant variable is the vector of mean residuals for assigned cases *meanoption_a* and the independant variable is the vector of attorney-specific residual assigned counsel penaltys, *attybeta*, calculated in the steps 1 and 2. 
Before we run the regression, we subset our data such that it only includes each attorney ones. 
Finally, save the coefficients from the regression.

```{r "4__9"}
# Filter Data (one row for each attorney)
Lawyers <- Lawyers %>% filter(atty_n==1)

# Run regression
wls <- lm(mean_resid_appt ~ attybeta, data = Lawyers, weights = shr_assigned_i)

# Extract Estimates
beta0 <- wls$coefficients[1]
beta1 <- wls$coefficients[2]

# Provide regression output
summary(wls)
```

**Task - Step 8:**
Click on `check` to show the plot.

```{r "4__10",results='asis', output='htmlwidget', widget='rbokeh', optional=TRUE}
figure(title="Attorney-Specific Assigned Counsel Penalties Impact on Indigent Client Recidivism", xlab="Residual Assigned Counsel Penalty", ylab="Probability of Recidivism", ylim=c(-0.5,1), width = 1500, height=1200, legend_location = NULL) %>%
  ly_points(x = attybeta, y = mean_resid_appt, data = Lawyers, color=experience, fill_color= c("#337ab7"), line_color="black", 
            alpha=0.5, size=shr_assigned_i, hover = list(attybarcard, attymale, atty_exp)) %>% ly_abline(wls, width = 2)

```

The plot shows the regression result, that we obtained from step 6. Accordingly, there is a positive and significant relationship between the attorney-specific assigned counsel penalty and the residualized recidivism of their assigned clients. Remember, that we computed the recidivism rate by first running the variable $recidivism3$ as response in step 3. The residuals, that we got thereafter in step 4 depict the left over variation in the error term after we accounted for case characteristics. We then took the mean within each attorney and over all cases with indigent accused. Hence the plot implies that, we can expect a client to face a higher probability of recidivism if he get assigned an attorney with relative high attorney-specific assigned counsel penalty. 


## Exercise 6 -- Conclusion


At first, let's sum up, what we learned about the Selection and Success of Lawyers. We saw, that attorneys who get assigned an indigent client achieve results that fall short compared to the outcomes which retained counsels achieve for their clients. Based on the ideas of the authors we considered four possible reasons that could explain this disparity: case characteristics, adverse selection, matching preferences and moral hazard. Thereupon we used the main model proposed by the authors, which we stated in equation (20) to assess the impact of case characteristics and adverse selection on the outcome, whereat we primarely focused on conviction as the oucome of interest. Thereby, we used the unique features of our panel data at hand to identify the residual assigned counsel penalty based on similar cases that lawyers resolved during a year as appointed and retained attorneys. Afterwards we implemented the Gelbach decomposition into our own function, to assess the disparity between the unconditional and conditional assigned counsel penalty and found that adverse selection plays a minor part compared to case characteristics. Although, case characteristics aren't solely responsible for the disparity. Finally, we investigated how matching preferences and moral hazard could help explain the residual assigned counsel penalty. While we concluded, that matching preferences play a negligible role, there was some evidence supporting the relevance of moral hazard to explain the remaining gap between retained and assigned attorneys. We found that appointed attorneys resolve cases faster which is driven by the compensation structure and contributes to the residual assigned counsel gap.

At second, during the exercises we learned basic data manipulation techniques with `dplyr` package. Further, we recalled some basic econometric theories abut OLS and applied them. We then turned to the unique properties of panel data and how to deal with it accordingly. We therefore worked our way through some theory about error component models and how to estimate them with fixed effects and whether this estimator yields causal results. Afterwards we saw how we can use the `felm()` function from the `lfe` package to estimate fixed effects models. Besides data manipulation and estimation we also explored data visualisation methods like the `ggplot2` package and `rbokeh`. 

I hope you enjoyed this problem set and gained some useful insights into the topic of selection and success of lawyers.


If you want to see the awards you earned during the problem set, you can run the following code chunk. 

```{r "6"}
awards()
```

## Exercise References 

### Bibliography

- *Abrams, D. S. & Yoon, A. H.* (2007). The luck of the draw: Using random case assignment to investigate attorney ability. The University of Chicago Law Review, 74(4), 1145-1177.
- *Angrist, J. D. & Pischke, J. S.* (2008): Mostly harmless econometrics: An empiricist's companion. Princeton university press.
- *Arellano, M.* (1987): Computing Robust Standard Errors for Within-Group Estimators, Oxford Bulletin of Economics and Statistics, 49, 431-434.
- *Arellano, M.* (2003): Panel data econometrics. Oxford university press.
- *Balazsi, L. Matyas, L. & Wansbeek, T.* (2018): The estimation of multidimensional fixed effects panel data models. Econometric Reviews, 37(3), 212-227.
- *Baltagi, B.* (2014): Econometric analysis of panel data. John Wiley & Sons.
- *Cameron, A. C. & Miller, D. L.* (2015): A practitioner’s guide to cluster-robust inference. Journal of human resources, 50(2), 317-372.
- *Cameron, A. C., Gelbach, J. B. & Miller, D. L.* (2008): Bootstrap-based improvements for inference with clustered errors. The Review of Economics and Statistics, 90(3), 414-427.
- *Cameron, A. C., Gelbach, J. B. & Miller, D. L.* (2011): Robust inference with multiway clustering. Journal of Business & Economic Statistics, 29(2), 238-249. 
- *Chang, W* (2020): R Graphics Cookbook. O'Reilly Media.
- *Cohen, T. H.* (2014): Who is better at defending criminals? Does type of defense attorney matter in terms of producing favorable case outcomes. Criminal Justice Policy Review, 25(1), 29-58.
- *Correia, S.* (2015): Singletons, cluster-robust standard errors and fixed effects: A bad mix. Technical Note, Duke University.
- *Croissant, Y. & Millo, G.* (2019): Panel data econometrics with R. John Wiley and Sons, Incorporated.
- *Gaure, S.* (2013a): lfe: Linear group fixed effects. The R Journal, 5(2), 104-117.
- *Gaure, S.* (2013b): OLS with multiple high dimensional category variables. Computational Statistics & Data Analysis, 66, 8-18.
- *Gelbach, J. B.* (2016): When do covariates matter? And which ones, and how much?, Journal of Labor Economics, 34(2), 509-543.
- *Gideon v. Wainwright*, 372 U.S. 335 (1963).
- *Greene, W. H.* (2003): Econometric analysis. Pearson Education India.
- *Holland, P.* (1986): Statistics and causal inference (with comments). Journal of the American Statistical Association, 81, 945-970.
- *Hsiao, C.* (2014): Analysis of panel data. Cambridge university press.
- *Imai, K. & Kim, I. S.* (2019): When should we use unit fixed effects regression models for causal inference with longitudinal data?. American Journal of Political Science, 63(2), 467-490.
- *Iyengar, R* (2007): An Analysis of the Performance of Federal Indigent Counsel. NBER Working Paper No. 13187, [http://www.nber.org/papers/w13187](http://www.nber.org/papers/w13187).
- *Liang, K.-Y. & S.L. Zeger* (1986): Longitudinal Data Analysis Using Generalized Linear Models, Biometrika, 73, 13-22.
- *Morgan, S. L. & Winship, C.* (2015): Counterfactuals and causal inference, Cambridge University Press.
- *Moss, C. B.*, (2014): Mathematical Statistics for Applied Econometrics. Chapman and Hall/CRC.
- *Moulton, B.R.* (1986): Random Group Effects and the Precision of Regression Estimates, Journal of Econometrics, 32, 385-397.
- *Moulton, B.R.* (1990): “An Illustration of a Pitfall in Estimating the Effects of Aggregate Variables on Micro Units,” Review of Economics and Statistics, 72, 334-38.
- *Newey, W. K. & Kenneth, D. West.* (1987): A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix, 55. Econometrica, 703-8.
- *Neyman, J.* (1923/1990): On the application of probability theory to agricultural experiments. Essay on principles. Section 9. Statistical Science, 5, 465-472.
- *Prabhakaran, S* (2016): r-statistics.co. http://r-statistics.co.
- *Roach, M. A.* (2014): Indigent defense counsel, attorney quality, and defendant outcomes. American Law and Economics Review, 16(2), 577-619.
- *Rosenbaum, P. R.* (1984): From association to causation in observational studies: The role of tests of strongly ignorable treatment assignment. Journal of the American Statistical Association, 79(385), 41-48.
- *Rubin, D. B.* (1973a): The use of matched sampling and regression adjustment to remove bias in observational studies. Biometrics, 185-203.
- *Rubin, D. B.* (1973b): Matching to remove bias in observational studies. Biometrics, 159-183.
- *Rubin, D. B.* (1974): Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology, 66(5), 688-701.
- *Rubin, D. B.* (1977): Assignment to treatment group on the basis of a covariate. Journal of educational Statistics, 2(1), 1-26.
- *Somaini, P. & Wolak, F. A.* (2016): An algorithm to estimate the two-way fixed effects model. Journal of Econometric Methods, 5(1), 143-152.
- *White, H.* (1980): A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica: journal of the Econometric Society, 817-838.
- *White, H.* (1984): Asymptotic Theory for Econometricians, Academic Press.
- *Wooldridge, J,* (2015): Introductory Econometrics: A Modern Approach, Cengage Learning.


### R Packages and other Programming Tools

- *Kharlampidi, V* (2020): swiper version 5.4.1, https://swiperjs.com. 
- *Gaure, S.* (2019): lfe: Linear Group Fixed Effects. https://CRAN.R-project.org/package=lfe.
- *Hlavac, M.* (2018): stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2.1. https://CRAN.R-project.org/package=stargazer.
- *Wickham, H.* (2016): ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York. ISBN 978-3-319-24277-4, https://ggplot2.tidyverse.org.
- *Wickham, H., Francois, R., Henry, L. & Muller, K.* (2018): dplyr. "A Grammar of Data Manipulation", R package version 0.7.7, http://CRAN.R-project.org/package=dplyr.
- *R Development Core Team* (2015): R. "A language and environment for statistical computing", R Foundation for Statistical Computing, Vienna, Austria, http://www.r-project.org.
- *Kranz, S.* (2015): RTutor. "Creating R problem sets with automatic assessment of student's solutions", R package version 2015.12.16, https://github.com/skranz/RTutor.
- *Hao Zhu* (2019): kableExtra: Construct Complex Table with 'kable' and Pipe Syntax. R package version 1.1.0. https://CRAN.R-project.org/package=kableExtra.
- *Wickham, H. and Henry, L.* (2020): tidyr: Tidy Messy Data. R package version 1.0.2. https://CRAN.R-project.org/package=tidyr.
- *Xie, Y* (2020): knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.28.
- *Dowle, M. & Srinivasan, A.* (2019): data.table: Extension of `data.frame`. R package version 1.12.8. https://CRAN.R-project.org/package=data.table.
- *Brown, C.* (2012): dummies: Create dummy/indicator variables flexibly and efficiently. R package version 1.5.6. https://CRAN.R-project.org/package=dummies.
- *Comtois, D.* (2020): summarytools: Tools to Quickly and Neatly Summarize Data. R package version 0.9.6. https://CRAN.R-project.org/package=summarytools.
- *Hafen, R. & Continuum Analytics, Inc.* (2016): rbokeh: R Interface for Bokeh. R package version 0.5.0. https://CRAN.R-project.org/package=rbokeh. 
- *Chang, W* (2018): shinythemes: Themes for Shiny. R package version 1.1.2. https://CRAN.R-project.org/package=shinythemes.

